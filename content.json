{"meta":{"title":"linjunhua's Blog","subtitle":"","description":"","author":"linlinnn","url":"https://github.com/linlinnn","root":"/"},"pages":[{"title":"About","date":"2020-01-18T12:56:07.352Z","updated":"2020-01-18T12:56:07.352Z","comments":true,"path":"about/index.html","permalink":"https://github.com/linlinnn/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-01-11T14:37:31.000Z","updated":"2020-01-11T14:37:31.921Z","comments":true,"path":"categories/index.html","permalink":"https://github.com/linlinnn/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-01-19T16:29:03.090Z","updated":"2020-01-19T16:29:03.090Z","comments":true,"path":"tags/index.html","permalink":"https://github.com/linlinnn/tags/index.html","excerpt":"","text":""},{"title":"Project","date":"2020-01-19T13:29:02.209Z","updated":"2020-01-18T12:56:07.380Z","comments":true,"path":"project/index.html","permalink":"https://github.com/linlinnn/project/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring 事务传播","slug":"Spring 事务传播","date":"2020-04-12T15:19:32.150Z","updated":"2020-04-12T16:14:11.405Z","comments":true,"path":"2020/04/12/Spring 事务传播/","link":"","permalink":"https://github.com/linlinnn/2020/04/12/Spring%20%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD/","excerpt":"","text":"Spring 事务传播Spring有七种事务传播级别，默认是REQUIRED 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public enum Propagation &#123; /** * 使用当前的事务，如果当前没有事务，则自己新建一个事务，子方法是必须运行在一个事务中的； * 如果当前存在事务，则加入这个事务，成为一个整体。 * &lt;p&gt;This is the default setting of a transaction annotation. */ REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED), /** * 如果当前有事务，则使用事务；如果当前没有事务，则不使用事务。 * 一般用于查询 */ SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS), /** * 该传播属性强制必须存在一个事务，如果不存在，则抛出异常 */ MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY), /** * 如果当前有事务，则挂起该事务，并且自己创建一个新的事务给自己使用； * 如果当前没有事务，则同 REQUIRED */ REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW), /** * 如果当前有事务，则把事务挂起，自己不使用事务去运行数据库操作 */ NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED), /** * 如果当前有事务存在，则抛出异常 */ NEVER(TransactionDefinition.PROPAGATION_NEVER), /** * 如果当前有事务，则开启子事务（嵌套事务），嵌套事务是独立提交或者回滚； * 如果当前没有事务，则同 REQUIRED。 * 但是如果主事务提交，则会携带子事务一起提交。 * 如果主事务回滚，则子事务会一起回滚。相反，子事务异常，则父事务可以回滚或不回滚。 */ NESTED(TransactionDefinition.PROPAGATION_NESTED); private final int value; Propagation(int value) &#123; this.value = value; &#125; public int value() &#123; return this.value; &#125;&#125;","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://github.com/linlinnn/tags/Spring/"}]},{"title":"Scala 数据类型","slug":"Scala 数据类型","date":"2020-04-11T15:08:26.415Z","updated":"2020-04-11T15:12:34.708Z","comments":true,"path":"2020/04/11/Scala 数据类型/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/Scala%20%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"Scala 数据类型方法和函数方法和函数的区别 方法是隶属于类或者对象的，在运行时，它是加载到JVM的方法区中 可以将函数对象赋值给一个变量，在运行时，它是加载到JVM的堆内存中 函数是一个对象，继承自FunctionN，函数对象有apply，curried，toString，tupled这些方法，而方法则没有 最多是Function22 即最多只能有22个参数 方法转换为函数 有时候需要将方法转换为函数，作为变量传递，就需要将方法转换为函数 使用_即可将方法转换为函数 12345scala&gt; def add(x:Int,y:Int)=x+yadd: (x: Int, y: Int)Intscala&gt; val a = add _a: (Int, Int) =&gt; Int = &lt;function2&gt; 数组定长数组 定长数组指的是数组的长度是不允许改变的 数组的元素是可以改变的 12345// 通过指定长度定义数组val/var 变量名 = new Array[元素类型](数组长度)// 用元素直接初始化数组val/var 变量名 = Array(元素1, 元素2, 元素3...) 在scala中，数组的泛型使用[]来指定使用()来获取元素 1234567891011121314151617181920scala&gt; val a=new Array[Int](10)a: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)scala&gt; a(0)res19: Int = 0scala&gt; a(0)=10scala&gt; ares21: Array[Int] = Array(10, 0, 0, 0, 0, 0, 0, 0, 0, 0)//////////////////////////////////////////////////////////////////scala&gt; val b = Array(\"hadoop\",\"spark\",\"hive\")b: Array[String] = Array(hadoop, spark, hive)scala&gt; b(0)res24: String = hadoopscala&gt; b.lengthres25: Int = 3 变长数组 变长数组指的是数组的长度是可变的，可以往数组中添加、删除元素 创建变长数组，需要提前导入ArrayBuffer类 1234567import scala.collection.mutable.ArrayBuffer// 创建空的ArrayBuffer变长数组val/var a = ArrayBuffer[元素类型]()// 创建带有初始元素的ArrayBuffer val/var a = ArrayBuffer(元素1，元素2，元素3....) 变长数组的增删改操作 使用+=添加元素 使用-=删除元素 使用++=追加一个数组到变长数组 123456789101112131415// 定义变长数组scala&gt; val a = ArrayBuffer(\"hadoop\", \"spark\", \"flink\")a: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, spark, flink)// 追加一个元素scala&gt; a += \"flume\"res10: a.type = ArrayBuffer(hadoop, spark, flink, flume)// 删除一个元素scala&gt; a -= \"hadoop\"res11: a.type = ArrayBuffer(spark, flink, flume)// 追加一个数组scala&gt; a ++= Array(\"hive\", \"sqoop\")res12: a.type = ArrayBuffer(spark, flink, flume, hive, sqoop) 遍历数组可以使用以下两种方式来遍历数组： 使用 for表达式 直接遍历数组中的元素 使用 索引 遍历数组中的元素 1234567891011121314151617scala&gt; for(i &lt;- a) println(i)hadoophiveflumesparkscala&gt; for(i &lt;- 0 to a.length -1 ) println(a(i))hadoophiveflumesparkscala&gt; for(i &lt;- 0 until a.length) println(a(i))hadoophiveflumespark 0 to n =&gt; [0, n] 0 until n =&gt; [0. n) 数组常用操作scala中的数组封装了丰富的计算操作，将来在对数据处理的时候，不需要我们自己再重新实现。 求和——sum方法 求最大值——max方法 求最小值——min方法 排序——sorted方法 12345678910111213141516171819202122scala&gt; val array = Array(1,3,4,2,5)array: Array[Int] = Array(1, 3, 4, 2, 5)//求和scala&gt; array.sumres10: Int = 15//求最大值scala&gt; array.maxres11: Int = 5//求最小值scala&gt; array.minres12: Int = 1//升序scala&gt; array.sortedres13: Array[Int] = Array(1, 2, 3, 4, 5)//降序 reverse 反转scala&gt; array.sorted.reverseres14: Array[Int] = Array(5, 4, 3, 2, 1) 元组元组可以用来包含一组不同类型的值。例如：姓名，年龄，性别，出生年月 元组的元素是不可变的 12345678910111213141516171819202122// 使用括号来定义元组val/var 元组变量名称 = (元素1, 元素2, 元素3....)// 使用箭头来定义元素（元组只有两个元素）val/var 元组 = 元素1-&gt;元素2scala&gt; val a = (1, \"张三\", 20, \"北京市\") a: (Int, String, Int, String) = (1,张三,20,北京市)scala&gt; val b = 1-&gt;2 b: (Int, Int) = (1,2)// 访问元组_1、_2、_3....// 获取元组中的第一个元素scala&gt; a._1res18: Int = 1//不能修改元组中的值scala&gt; a._4=\"上海\"&lt;console&gt;:12: error: reassignment to val a._4=\"上海\" ^ 映射Map Map可以称之为映射。它是由键值对组成的集合。scala当中的Map集合与java当中的Map类似，也是key，value对形式的。 在scala中，Map也分为不可变Map和可变Map。 不可变Map1234567891011val/var map = Map(键-&gt;值, 键-&gt;值, 键-&gt;值...) // 推荐，可读性更好 val/var map = Map((键, 值), (键, 值), (键, 值), (键, 值)...)scala&gt; val map1 = Map(\"zhangsan\"-&gt;30, \"lisi\"-&gt;40) map: scala.collection.immutable.Map[String,Int] = Map(zhangsan -&gt; 30, lisi -&gt; 40)scala&gt; val map2 = Map((\"zhangsan\", 30), (\"lisi\", 30)) map: scala.collection.immutable.Map[String,Int] = Map(zhangsan -&gt; 30, lisi -&gt; 30)// 根据key获取value scala&gt; map1(\"zhangsan\") res10: Int = 30 可变Map可变Map需要手动导入import scala.collection.mutable.Map, 定义语法与不可变Map一致 123456789101112131415161718//导包scala&gt; import scala.collection.mutable.Mapimport scala.collection.mutable.Map//定义可变的mapscala&gt; val map3 = Map(\"zhangsan\"-&gt;30, \"lisi\"-&gt;40)map3: scala.collection.mutable.Map[String,Int] = Map(lisi -&gt; 40, zhangsan -&gt; 30)//获取zhangsan这个key对应的valuescala&gt; map3(\"zhangsan\")res26: Int = 30//给zhangsan这个key重新赋值valuescala&gt; map3(\"zhangsan\")=50//显示map3scala&gt; map3res28: scala.collection.mutable.Map[String,Int] = Map(lisi -&gt; 40, zhangsan -&gt; 50) Map基本操作123456789101112131415161718192021222324252627282930// 获取zhangsan的年龄 scala&gt; map(\"zhangsan\")res10: Int = 30// 获取wangwu的年龄，如果wangwu不存在，则返回-1 比较友好，避免遇到不存在的key而报错scala&gt; map.getOrElse(\"wangwu\", -1) res11: Int = -1// 修改key对应的valuescala&gt; map(\"lisi\")=50// 添加key-value键值对scala&gt; map+=(\"wangwu\" -&gt;35)res12: map.type = Map(lisi -&gt; 50, zhangsan -&gt; 30, wangwu -&gt; 35)// 删除key-value键值对scala&gt; map -=\"wangwu\"res13: map.type = Map(lisi -&gt; 50, zhangsan -&gt; 30)//获取所有的keyscala&gt; map.keysres36: Iterable[String] = Set(lisi, zhangsan)//获取所有的keyscala&gt; map.keySetres37: scala.collection.Set[String] = Set(lisi, zhangsan)//获取所有的valuescala&gt; map.valuesres38: Iterable[Int] = HashMap(50, 30) 遍历map12345678910//第一种遍历scala&gt; for(k &lt;- map.keys) println(k + \" -&gt; \" + map(k))lisi -&gt; 50zhangsan -&gt; 30//第二种遍历scala&gt; for((k,v) &lt;- map) println(k + \" -&gt; \" + v) lisi -&gt; 50zhangsan -&gt; 30 Set集合 Set是代表没有重复元素的集合。 Set具备以下性质： 1、元素不重复 2、不保证插入顺序 scala中的集也分为两种，一种是不可变集合，另一种是可变集合。 不可变Set集合123456789101112131415161718192021222324252627282930313233343536//创建一个空的不可变集val/var 变量名 = Set[类型]()//给定元素来创建一个不可变集val/var 变量名 = Set[类型](元素1, 元素2, 元素3...)// 创建set集合 scala&gt; val a = Set(1,1,2,3,4,5) a: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4)// 获取集合的大小 scala&gt; a.size res0: Int = 5// 遍历集合scala&gt; for(i &lt;- a) println(i)//添加元素生成新的集合scala&gt; a + 6res1: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 3, 4)// 删除一个元素 scala&gt; a - 1 res2: scala.collection.immutable.Set[Int] = Set(5, 2, 3, 4)// 删除set集合中存在的元素 scala&gt; a -- Set(2,3) res3: scala.collection.immutable.Set[Int] = Set(5, 1, 4)// 拼接两个集合 scala&gt; a ++ Set(6,7,8) res4: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 7, 3, 8, 4)//求2个Set集合的交集scala&gt; a &amp; Set(3,4,5,6)res5: scala.collection.immutable.Set[Int] = Set(5, 3, 4) 注意：这里对不可变的set集合进行添加删除等操作，对于该集合来说是没有发生任何变化，这里是生成了新的集合，新的集合相比于原来的集合来说发生了变化 可变Set集合12345678910111213141516171819202122232425262728293031323334353637//导包scala&gt; import scala.collection.mutable.Setimport scala.collection.mutable.Set//定义可变的set集合scala&gt; val set=Set(1,2,3,4,5)set: scala.collection.mutable.Set[Int] = Set(1, 5, 2, 3, 4)//添加单个元素scala&gt; set +=6res10: set.type = Set(1, 5, 2, 6, 3, 4)//添加多个元素scala&gt; set += (6,7,8,9)res11: set.type = Set(9, 1, 5, 2, 6, 3, 7, 4, 8)//添加一个set集合中的元素scala&gt; set ++=Set(10,11)res12: set.type = Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 11, 8)//删除一个元素scala&gt; set -=11res13: set.type = Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 8)//删除多个元素scala&gt; set -= (9,10)res15: set.type = Set(1, 5, 2, 6, 3, 7, 4, 8)//删除一个set子集scala&gt; set --= Set(7,8)res19: set.type = Set(1, 5, 2, 6, 3, 4)scala&gt; set.remove(1)res17: Boolean = truescala&gt; setres18: scala.collection.mutable.Set[Int] = Set(5, 2, 6, 3, 4) 列表 List List是scala中最重要的、也是最常用的数据结构 List具备以下性质： 1、可以保存重复的值 2、有先后顺序 在scala中，也有两种列表，一种是不可变列表、另一种是可变列表 不可变列表 不可变列表就是列表的元素、长度都是不可变的 语法 使用 List(元素1, 元素2, 元素3, …) 来创建一个不可变列表，语法格式 12345678910111213141516171819val/var 变量名 = List(元素1, 元素2, 元素3...)//使用 Nil 创建一个不可变的空列表val/var 变量名 = Nil//使用 :: 方法创建一个不可变列表val/var 变量名 = 元素1 :: 元素2 :: Nil//创建一个不可变列表，存放以下几个元素（1,2,3,4）scala&gt; val list1 = List(1, 2, 3, 4)list1: List[Int] = List(1, 2, 3, 4)//使用Nil创建一个不可变的空列表scala&gt; val list2 = Nillist2: scala.collection.immutable.Nil.type = List()//使用 :: 方法创建列表，包含1、2、3三个元素scala&gt; val list3 = 1::2::3::Nillist3: List[Int] = List(1, 2, 3) 可变列表 可变列表就是列表的元素、长度都是可变的。 要使用可变列表，先要导入 import scala.collection.mutable.ListBuffer 1234567891011//导包scala&gt; import scala.collection.mutable.ListBufferimport scala.collection.mutable.ListBuffer//定义一个空的可变列表scala&gt; val a=ListBuffer[Int]()a: scala.collection.mutable.ListBuffer[Int] = ListBuffer()//定义一个有初始元素的可变列表scala&gt; val b=ListBuffer(1,2,3,4)b: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4) 列表操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//导包scala&gt; import scala.collection.mutable.ListBufferimport scala.collection.mutable.ListBuffer//定义一个可变的列表scala&gt; val list=ListBuffer(1,2,3,4)list: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4)//获取第一个元素scala&gt; list(0)res4: Int = 1//获取第一个元素scala&gt; list.headres5: Int = 1//获取除了第一个元素外其他元素组成的列表scala&gt; list.tailres6: scala.collection.mutable.ListBuffer[Int] = ListBuffer(2, 3, 4)//添加单个元素scala&gt; list +=5res7: list.type = ListBuffer(1, 2, 3, 4, 5)//添加一个不可变的列表scala&gt; list ++=List(6,7)res8: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7)//添加一个可变的列表scala&gt; list ++=ListBuffer(8,9)res9: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7, 8, 9)//删除单个元素scala&gt; list -=9res10: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7, 8)//删除一个不可变的列表存在的元素scala&gt; list --= List(7,8)res11: list.type = ListBuffer(1, 2, 3, 4, 5, 6)//删除一个可变的列表存在的元素scala&gt; list --= ListBuffer(5,6)res12: list.type = ListBuffer(1, 2, 3, 4)//可变的列表转为不可变列表scala&gt; list.toListres13: List[Int] = List(1, 2, 3, 4)//可变的列表转为不可变数组scala&gt; list.toArrayres14: Array[Int] = Array(1, 2, 3, 4)","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://github.com/linlinnn/tags/Scala/"}]},{"title":"Scala 面对对象编程","slug":"Scala 面对对象编程","date":"2020-04-11T15:08:26.400Z","updated":"2020-04-11T15:14:19.403Z","comments":true,"path":"2020/04/11/Scala 面对对象编程/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/Scala%20%E9%9D%A2%E5%AF%B9%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/","excerpt":"","text":"Scala 面对对象编程类类的定义scala是支持面向对象的，也有类和对象的概念 定义一个Customer类，并添加成员变量/成员方法 添加一个main方法，并创建Customer类的对象，并给对象赋值，打印对象中的成员，调用成员方法 12345678910111213141516171819202122class Customer &#123; var name:String = _ var sex:String = _ val registerDate:Date = new Date def sayHi(msg:String) = &#123; println(msg) &#125;&#125;object Main &#123; def main(args: Array[String]): Unit = &#123; val customer = new Customer //给对象的成员变量赋值 customer.name = \"张三\" customer.sex = \"男\" println(s\"姓名: $&#123;customer.name&#125;, 性别：$&#123;customer.sex&#125;, 注册时间: $&#123;customer.registerDate&#125;\") //对象调用方法 customer.sayHi(\"你好!\") &#125;&#125; var name:String = _， _表示使用默认值进行初始化 例如：String类型默认值是null，Int类型默认值是0，Boolean类型默认值是false… val变量不能使用_来进行初始化，因为val是不可变的，所以必须手动指定一个默认值 main方法必须要放在一个scala的object（单例对象）中才能执行 类的构造器主构造器 主构造器是指在类名的后面跟上一系列参数，例如 123class 类名(var/val 参数名:类型 = 默认值, var/val 参数名:类型 = 默认值)&#123; // 构造代码块&#125; 辅助构造器 在类中使用this来定义，例如 123def this(参数名:类型, 参数名:类型) &#123; ...&#125; 123456789101112class Student(val name:String, val age:Int) &#123; val address:String=\"beijing\" // 定义一个参数的辅助构造器 def this(name:String) &#123; // 第一行必须调用主构造器、其他辅助构造器或者super父类的构造器 this(name, 20) &#125; def this(age:Int) &#123; this(\"某某某\", age) &#125;&#125; 对象 scala中是没有Java中的静态成员的。如果将来我们需要用到static变量、static方法，就要用到scala中的单例对象object 定义object 定义单例对象和定义类很像，就是把class换成object 1234567891011121314151617object DateUtils &#123; // 在object中定义的成员变量，相当于Java中定义一个静态变量 // 定义一个SimpleDateFormat日期时间格式化对象 val simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\") // 构造代码 println(\"构造代码\") // 相当于Java中定义一个静态方法 def format(date:Date) = simpleDateFormat.format(date) // main是一个静态方法，所以必须要写在object中 def main(args: Array[String]): Unit = &#123; println &#123; DateUtils.format(new Date()) &#125;; &#125;&#125; (1). 使用object单例对象名定义一个单例对象，可以用object作为工具类或者存放常量(2). 在单例对象中定义的变量，类似于Java中的static成员变量(3). 在单例对象中定义的方法，类似于Java中的static方法(4). object单例对象的构造代码可以直接写在花括号中(5). 调用单例对象的方法，直接使用单例对象名.方法名，访问单例对象的成员变量也是使用单例对象名.变量名(6). 单例对象只能有一个无参的主构造器，不能添加其他参数 伴生对象 在同一个scala文件，有一个class和object具有同样的名字，那么就称这个object是class的伴生对象，class是object的伴生类； 伴生类和伴生对象的最大特点是，可以相互访问； 12345678910111213141516171819class ClassObject &#123; val id = 1 private var name = \"itcast\" def printName(): Unit =&#123; //在ClassObject类中可以访问伴生对象ClassObject的私有属性 println(ClassObject.CONSTANT + name ) &#125;&#125;object ClassObject&#123; //伴生对象中的私有属性 private val CONSTANT = \"汪汪汪 : \" def main(args: Array[String]) &#123; val p = new ClassObject //访问私有的字段name p.name = \"123\" p.printName() &#125;&#125; (1). 伴生类和伴生对象的名字必须是一样的(2). 伴生类和伴生对象需要在一个scala源文件中(3). 伴生类和伴生对象可以互相访问private的属性 object的apply方法12// 创建一个Array对象val a = Array(1,2,3,4) 这种写法非常简便，不需要再写一个new，然后敲一个空格，再写类名。如何直接使用类名来创建对象呢？ 实现伴生对象的apply方法 伴生对象的apply方法用来快速地创建一个伴生类的对象 123456789101112131415161718192021222324252627282930class Person(var name:String, var age:Int) &#123; override def toString = s\"Person($name, $age)\"&#125;object Person &#123; // 实现apply方法 // 返回的是伴生类的对象 def apply(name:String, age:Int): Person = new Person(name, age) // apply方法支持重载 def apply(name:String):Person = new Person(name, 20) def apply(age:Int):Person = new Person(\"某某某\", age) def apply():Person = new Person(\"某某某\", 20)&#125;object Main2 &#123; def main(args: Array[String]): Unit = &#123; val p1 = Person(\"张三\", 20) val p2 = Person(\"李四\") val p3 = Person(100) val p4 = Person() println(p1) println(p2) println(p3) println(p4) &#125;&#125; （1）当遇到类名(参数1, 参数2…)会自动调用apply方法，在apply方法中来创建对象（2）定义apply时，如果参数列表是空，也不能省略括号()，否则引用的是伴生对象 object的main方法 scala和Java一样，如果要运行一个程序，必须有一个main方法。 而在Java中main方法是静态的，而在scala中没有静态方法。 在scala中，这个main方法必须放在一个object中 12345object Main1&#123; def main(args:Array[String]) = &#123; println(\"hello, scala\") &#125;&#125; 也可以继承自App Trait（特质），然后将需要编写在main方法中的代码，写在object的构造方法体内。其本质是调用了Trait这个特质中的main方法。 123object Main2 extends App &#123; println(\"hello, scala\")&#125; trait特质​ 大多数场景下，Scala中的特质（trait）相当于Java中的接口，正如Java类能够实现多个接口一样，Scala 类可以继承多个特质，由于特质能够像Java中的抽象类那样包含自己已实现的方法，所以特质的功能要比 Java接口强大得多。然而，和Java中的抽象类不同，Scala可以把多个特质混入到一个类中，与此同时，一个 特质也能控制它混入的类。 1234567trait BaseSoundPlayer&#123; def play def close def pause def stop def resume&#125; 如果方法不需要任何参数，在def后指定方法名即可，如上所示。如果一个方法需要参数，只需要把它 们罗列出来即可 1234trait Dog&#123; def speak(whatToSay: String) def wagTail(enabled: Boolean)&#125; 继承特质当一个类需要继承特质时，要使用extends和with关键字。只继承一个特质时，使用extends 1class Mp3SoundPlayer extends BaseSoundPlayer 继承一个类和多个特质时 1class Foo extends BaseClass with trait1 with trait2 继承多个特质时 1class Foo extends trait1 with trait2 with trait3 除非实现特质的类是一个抽象类，否则必须实现特质的所有抽象方法 特质也可以继承另一个特质 特质的字段一个特质的字段可以声明为var或者val。在一个子类（或子特质）中覆写特质中的var字段时，不需要 override关键字，但当覆写一个特质中的val字段时，需要使用override关键字 1234567891011trait PizzaTrait&#123; var numToppings: Int //抽象字段，子类需要定义（除非为抽象类） var size = 14 val maxNumToppings = 10&#125;class Pizza extends PizzaTrait&#123; numToppings = 0 // 'override' and 'var' not needed size = 16 override val maxNumToppings = 15&#125; 模式匹配 scala有一个十分强大的模式匹配机制，可以应用到很多场合。 switch语句 类型查询 快速获取数据 并且scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配。 12345678910111213141516object CaseDemo01 extends App&#123; //定义一个数组 val arr = Array(\"hadoop\",\"zookeeper\",\"spark\",\"storm\") //随机取数组中的一位，使用Random.nextInt val name = arr(Random.nextInt(arr.length)) println(name) name match &#123; case \"hadoop\" =&gt; println(\"大数据分布式存储和计算框架...\") case \"zookeeper\" =&gt; println(\"大数据分布式协调服务框架...\") case \"spark\" =&gt; println(\"大数据分布式内存计算框架...\") //表示以上情况都不满足才会走最后一个 case _ =&gt; println(\"我不认识你\") &#125;&#125; 匹配类型123456789101112131415object CaseDemo02 extends App&#123; //定义一个数组 val arr = Array(\"hello\",1,-2.0,CaseDemo02) //随机获取数组中的元素 val value=arr(Random.nextInt(arr.length)) println(value) value match &#123; case x:Int =&gt; println(\"Int=&gt;\"+x) case y:Double if(y&gt;=0) =&gt; println(\"Double=&gt;\"+y) case z:String =&gt; println(\"String=&gt;\"+z) case _ =&gt; throw new Exception(\"not match exception\") &#125;&#125; 匹配数组1234567891011object CaseDemo03 extends App&#123; //匹配数组 val arr = Array(1,3,5) arr match&#123; case Array(1,x,y) =&gt; println(x+\"---\"+y) case Array(1,_*) =&gt; println(\"1...\") case Array(0) =&gt; println(\"only 0\") case _ =&gt; println(\"something else\") &#125;&#125; 匹配集合123456789object CaseDemo04 extends App&#123; val list=List(0,3,6) list match &#123; case 0::Nil =&gt; println(\"only 0\") case 0::tail =&gt; println(\"0....\") case x::y::z::Nil =&gt; println(s\"x:$x y:$y z:$z\") case _ =&gt; println(\"something else\") &#125;&#125; 匹配元组12345678object CaseDemo05 extends App&#123; val tuple = (1,3,5) tuple match&#123; case (1,x,y) =&gt; println(s\"1,$x,$y\") case (2,x,y) =&gt; println(s\"$x,$y\") case _ =&gt; println(\"others...\") &#125;&#125; 案例类案例类（Case classes）和普通类差不多，非常适合用于不可变的数据 123case class Book(isbn: String)val frankenstein = Book(\"978-0486282114\") 注意在实例化案例类Book时，并没有使用关键字new，这是因为案例类有一个默认的apply方法来负 责对象的创建。 当创建包含参数的案例类时，这些参数是公开（public）的val 12345case class Message(sender: String, recipient: String, body: String)val message1 = Message(\"guillaume@quebec.ca\", \"jorge@catalonia.es\", \"Ça va ?\")println(message1.sender) // prints guillaume@quebec.camessage1.sender = \"travis@washington.us\" // this line does not compile 不能给message1.sender重新赋值，因为它是一个val（不可变）。在案例类中使用var也是可以的，但 并不推荐这样。 比较案例类在比较的时候是按值比较而非按引用比较 12345case class Message(sender: String, recipient: String, body: String)val message2 = Message(\"jorge@catalonia.es\", \"guillaume@quebec.ca\", \"Com va?\")val message3 = Message(\"jorge@catalonia.es\", \"guillaume@quebec.ca\", \"Com va?\")val messagesAreTheSame = message2 == message3 // true 拷贝可以通过copy方法创建一个案例类实例的浅拷贝，同时可以指定构造参数来做一些改变 123456case class Message(sender: String, recipient: String, body: String)val message4 = Message(\"julien@bretagne.fr\", \"travis@washington.us\", \"Me zo o komz gant ma amezeg\")val message5 = message4.copy(sender = message4.recipient, recipient = \"claire@bourgogne.fr\")message5.sender // travis@washington.usmessage5.recipient // claire@bourgogne.frmessage5.body // \"Me zo o komz gant ma amezeg\"","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://github.com/linlinnn/tags/Scala/"}]},{"title":"Scala 函数式编程","slug":"Scala 函数式编程","date":"2020-04-11T15:08:26.380Z","updated":"2020-04-11T15:15:03.121Z","comments":true,"path":"2020/04/11/Scala 函数式编程/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/Scala%20%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/","excerpt":"","text":"Scala 函数式编程遍历 - foreach1foreach(f: (A) ⇒ Unit): Unit foreach API 说明 参数 f: (A) ⇒ Unit 接收一个函数对象函数的输入参数为集合的元素返回值为空 返回值 Unit 空 12345678//匿名函数的输入参数类型可以省略，由编译器自动推断scala&gt; list.foreach(x=&gt;println(x))//当函数参数，只在函数体中出现一次，而且函数体没有嵌套调用时，可以使用下划线来简化函数定义scala&gt; list.foreach(println(_))//最简写，直接给定printlnscala&gt; list.foreach(println) 映射 - map1def map[B](f: (A) ⇒ B): TraversableOnce[B] map方法 API 说明 泛型 [B] 指定map方法最终返回的集合泛型 参数 f: (A) ⇒ B 传入一个函数对象该函数接收一个类型A（要转换的列表元素）返回值为类型B 返回值 TraversableOnce[B] B类型的集合 1234567891011//定义一个list集合，实现把内部每一个元素做乘以10，生成一个新的list集合scala&gt; val list=List(1,2,3,4)list: List[Int] = List(1, 2, 3, 4)//省略匿名函数参数类型scala&gt; list.map(x=&gt;x*10)res22: List[Int] = List(10, 20, 30, 40)//最简写用下划线scala&gt; list.map(_*10)res23: List[Int] = List(10, 20, 30, 40) 扁平化映射 - flatmap1def flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): TraversableOnce[B] flatmap方法 API 说明 泛型 [B] 最终要转换的集合元素类型 参数 f: (A) ⇒ GenTraversableOnce[B] 传入一个函数对象函数的参数是集合的元素函数的返回值是一个集合 返回值 TraversableOnce[B] B类型的集合 123456789101112131415//定义一个List集合,每一个元素中就是一行数据，有很多个单词scala&gt; val list = List(\"hadoop hive spark flink\", \"hbase spark\")list: List[String] = List(hadoop hive spark flink, hbase spark)//使用flatMap进行扁平化处理，获取得到所有的单词scala&gt; list.flatMap(x =&gt; x.split(\" \"))res24: List[String] = List(hadoop, hive, spark, flink, hbase, spark)//简写scala&gt; list.flatMap(_.split(\" \"))res25: List[String] = List(hadoop, hive, spark, flink, hbase, spark)//flatMap该方法其本质是先进行了map 然后又调用了flattenscala&gt; list.map(_.split(\" \")).flattenres26: List[String] = List(hadoop, hive, spark, flink, hbase, spark) 过滤 - filter1def filter(p: (A) ⇒ Boolean): TraversableOnce[A] filter方法 API 说明 参数 p: (A) ⇒ Boolean 传入一个函数对象接收一个集合类型的参数返回布尔类型，满足条件返回true, 不满足返回false 返回值 TraversableOnce[A] 列表 1234567891011//定义一个list集合scala&gt; val list = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)list: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)//过滤出集合中大于5的元素scala&gt; list.filter(x =&gt; x&gt;5)res27: List[Int] = List(6, 7, 8, 9, 10)//把集合中大于5的元素取出来乘以10生成一个新的list集合scala&gt; list.filter(_ &gt; 5).map(_ * 10)res29: List[Int] = List(60, 70, 80, 90, 100) 排序 - sort在scala集合中，可以使用以下几种方式来进行排序 sorted默认排序 sortBy指定字段排序 sortWith自定义排序 sorted默认排序1234567//定义一个List集合scala&gt; val list=List(5,1,2,4,3)list: List[Int] = List(5, 1, 2, 4, 3)//默认就是升序scala&gt; list.sortedres30: List[Int] = List(1, 2, 3, 4, 5) sortBy指定字段排序1def sortBy[B](f: (A) ⇒ B): List[A] sortBy方法 API 说明 泛型 [B] 按照什么类型来进行排序 参数 f: (A) ⇒ B 传入函数对象接收一个集合类型的元素参数返回B类型的元素进行排序 返回值 List[A] 返回排序后的列表 1234567//定义一个List集合scala&gt; val list=List(\"1 hadoop\",\"2 spark\",\"3 flink\")list: List[String] = List(1 hadoop, 2 spark, 3 flink)//按照单词的首字母进行排序scala&gt; list.sortBy(x=&gt;x.split(\" \")(1))res33: List[String] = List(3 flink, 1 hadoop, 2 spark) sortWith自定义排序1def sortWith(lt: (A, A) ⇒ Boolean): List[A] sortWith方法 API 说明 参数 lt: (A, A) ⇒ Boolean 传入一个比较大小的函数对象接收两个集合类型的元素参数返回两个元素大小，小于返回true，大于返回false 返回值 List[A] 返回排序后的列表 12345678910scala&gt; val list = List(2,3,1,6,4,5)a: List[Int] = List(2, 3, 1, 6, 4, 5)//降序scala&gt; list.sortWith((x,y)=&gt;x&gt;y)res35: List[Int] = List(6, 5, 4, 3, 2, 1)//升序scala&gt; list.sortWith((x,y)=&gt;x&lt;y)res36: List[Int] = List(1, 2, 3, 4, 5, 6) 分组 - groupBy1def groupBy[K](f: (A) ⇒ K): Map[K, List[A]] groupBy方法 API 说明 泛型 [K] 分组字段的类型 参数 f: (A) ⇒ K 传入一个函数对象接收集合元素类型的参数返回一个K类型的key，这个key会用来进行分组，相同的key放在一组中 返回值 Map[K, List[A]] 返回一个映射，K为分组字段，List为这个分组字段对应的一组数据 1234567891011scala&gt; val a = List(\"张三\"-&gt;\"男\", \"李四\"-&gt;\"女\", \"王五\"-&gt;\"男\")a: List[(String, String)] = List((张三,男), (李四,女), (王五,男))// 按照性别分组scala&gt; a.groupBy(_._2)res0: scala.collection.immutable.Map[String,List[(String, String)]] = Map(男 -&gt; List((张三,男), (王五,男)),女 -&gt; List((李四,女)))// 将分组后的映射转换为性别/人数元组列表scala&gt; res0.map(x =&gt; x._1 -&gt; x._2.size)res3: scala.collection.immutable.Map[String,Int] = Map(男 -&gt; 2, 女 -&gt; 1) 聚合 - reduce1def reduce[A1 &gt;: A](op: (A1, A1) ⇒ A1): A1 reduce方法 API 说明 泛型 [A1 &gt;: A] （下界）A1必须是集合元素类型的子类 参数 op: (A1, A1) ⇒ A1 传入函数对象，用来不断进行聚合操作第一个A1类型参数为：当前聚合后的变量第二个A1类型参数为：当前要进行聚合的元素 返回值 A1 列表最终聚合为一个元素 123456789101112131415161718scala&gt; val a = List(1,2,3,4,5,6,7,8,9,10)a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; a.reduce((x,y) =&gt; x + y)res5: Int = 55// 第一个下划线表示第一个参数，就是历史的聚合数据结果// 第二个下划线表示第二个参数，就是当前要聚合的数据元素scala&gt; a.reduce(_ + _)res53: Int = 55// 与reduce一样，从左往右计算scala&gt; a.reduceLeft(_ + _)res0: Int = 55// 从右往左聚合计算scala&gt; a.reduceRight(_ + _)res1: Int = 55 折叠 - fold1def fold[A1 &gt;: A](z: A1)(op: (A1, A1) ⇒ A1): A1 reduce方法 API 说明 泛型 [A1 &gt;: A] （下界）A1必须是集合元素类型的子类 参数1 z: A1 初始值 参数2 op: (A1, A1) ⇒ A1 传入函数对象，用来不断进行折叠操作第一个A1类型参数为：当前折叠后的变量第二个A1类型参数为：当前要进行折叠的元素 返回值 A1 列表最终折叠为一个元素 12345678910111213141516171819202122//定义一个List集合scala&gt; val a = List(1,2,3,4,5,6,7,8,9,10)a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)//求和scala&gt; a.sumres41: Int = 55//给定一个初始值，，折叠求和scala&gt; a.fold(0)(_+_)res42: Int = 55scala&gt; a.fold(10)(_+_)res43: Int = 65//从左往右scala&gt; a.foldLeft(10)(_+_)res44: Int = 65//从右往左scala&gt; a.foldRight(10)(_+_)res45: Int = 65 高阶函数使用函数值作为参数，或者返回值为函数值的“函数”和“方法”，均称之为“高阶函数” 函数值作为参数1234567891011//定义一个数组scala&gt; val array=Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)//定义一个函数scala&gt; val func=(x:Int)=&gt;x*10func: Int =&gt; Int = &lt;function1&gt;//函数作为参数传递到方法中scala&gt; array.map(func)res0: Array[Int] = Array(10, 20, 30, 40, 50) 匿名函数1234567//定义一个数组scala&gt; val array = Array(1, 2, 3, 4, 5)array: Array[Int] = Array(1, 2, 3, 4, 5)//定义一个没有名称的函数----匿名函数scala&gt; array.map(x=&gt;x*10)res1: Array[Int] = Array(10, 20, 30, 40, 50) 柯里化方法可以定义多个参数列表，当使用较少的参数列表调用多参数列表的方法时，会产生一个新的函数，该函数接收剩余的参数列表作为其参数。这被称为柯里化。 12345678910111213141516171819202122def getAddress(a:String):(String,String)=&gt;String=&#123; (b:String,c:String)=&gt;a+\"-\"+b+\"-\"+c&#125;scala&gt; val f1 = getAddress(\"china\")f1: (String, String) =&gt; String = &lt;function2&gt;scala&gt; f1(\"beijing\",\"tiananmen\")res5: String = china-beijing-tiananmen//这里就可以这样去定义方法def getAddress(a:String)(b:String,c:String):String=&#123; a+\"-\"+b+\"-\"+c &#125;//调用scala&gt; getAddress(\"china\")(\"beijing\",\"tiananmen\")res0: String = china-beijing-tiananmen//之前使用的下面这些操作就是使用到了柯里化List(1,2,3,4).fold(0)(_+_)List(1,2,3,4).foldLeft(0)(_+_)List(1,2,3,4).foldRight(0)(_+_) 闭包函数里面引用外面类成员变量叫作闭包 123var factor=10val f1=(x:Int) =&gt; x*factor 定义的函数f1，它的返回值是依赖于不在函数作用域的一个变量 后期必须要要获取到这个变量才能执行 spark和flink程序的开发中大量的使用到函数，函数的返回值依赖的变量可能都需要进行大量的网络传 输获取得到，这里就需要这些变量实现序列化进行网络传输。","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://github.com/linlinnn/tags/Scala/"}]},{"title":"MySQL 索引","slug":"MySQL 索引","date":"2020-04-11T15:08:26.369Z","updated":"2020-04-11T15:12:02.184Z","comments":true,"path":"2020/04/11/MySQL 索引/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/MySQL%20%E7%B4%A2%E5%BC%95/","excerpt":"","text":"MySQL 索引索引的基本评估思路B+树 页大小：16KB 如何计算对应树的高度，表能存放的记录数 12show table status like &#39;table_a&#39;;# 参考对应的avg_row_length 现在假设主键为10个字节，每条记录平均500字节 聚簇索引，记录存放在叶子节点 树的高度 记录数 1 32 2 1000 * 32 = 32000 3 1000^2 * 32 = 32000000 MySQL 数据量级为千万级别的时候，一般能有比较不错的查询性能 当然这是理论值，因为会有碎片的存在，可以乘以70%作为合理预估 IOPS = 1000ms / (寻道时间 + 旋转延迟) 磁盘的IOPS大约为100，所以一个树高为3的查询时间可预估为3 / 100 = 0.03 s 查看索引信息1show index from table_a Cardinality 是一个比较重要的指标，表示不重复值的个数（采样预估，具体采样页数可以控制），这个值越大越好，用来衡量索引区分数据的能力 如果是复合索引，则后面的Cardinality表示的是和前面的sequence的列组合起来的区分度 即seq为1，表示一个列的区分度，seq为2，表示1和2列组合起来的区分度 思考：索引应该创建在区分度比较高的列，那么类别需不需要加索引？ 一般而言，类别的种类不会很多，所以区分度并不高，那么为了支持类别的查询，可以考虑使用分区的方式 复合索引由多个列组成的索引，最重要的就是最左前缀原则 索引能进行快速定位数据的原理是，对组成索引的列进行了排序 假设table_a有复合索引(a, b) 123456789101112# 可以使用索引select * from table_a where a &#x3D; ?# 可以使用索引select * from table_a where a &#x3D; ? and b &#x3D; ?# 不可以使用索引select * from table_a where b &#x3D; ?# 不可以使用索引select * from table_a where a &#x3D; ? or b &#x3D; ?# 先使用索引a得到一部分数据，再用b &#x3D; ?进行过滤select * from table_a where a &gt; ? and b &#x3D; ?# 可以使用覆盖索引select count(1) from table_a where b &gt; ? and b &lt; ? 假设table_a有复合索引(a, b, c) 12# 先使用索引a得到一部分数据，再用c &#x3D; ?进行过滤select * from table_a where a &#x3D; ? and c &#x3D; ?","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://github.com/linlinnn/tags/MySQL/"}]},{"title":"MySQL 数据类型","slug":"MySQL 数据类型","date":"2020-04-11T15:08:26.348Z","updated":"2020-04-11T15:11:22.799Z","comments":true,"path":"2020/04/11/MySQL 数据类型/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/MySQL%20%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"MySQL 数据类型整型 Type Storage (Bytes) Minimum Value Signed Minimum Value Unsigned Maximum Value Signed Maximum Value Unsigned TINYINT 1 -128 0 127 255 SMALLINT 2 -32768 0 32767 65535 MEDIUMINT 3 -8388608 0 8388607 16777215 INT 4 -2147483648 0 2147483647 4294967295 BIGINT 8 -2^63 0 2^63-1 2^64-1 自增主键推荐使用bigint int类型推荐使用默认signed，一则数据量级不足可选择bigint，二则避免小-大会越界的问题 整数类型的(N)，N表示的和数字范围无关，如果使用zerofill的话，不足的位会填0 定点型（精确数字型）12salary DECIMAL(5,2)#-999.99 to 999.99 maximum number of digits for DECIMAL is 65，默认是DECIMAL(10, 0) 16位就已经到千兆级别了，所以定义金钱类型可以使用DECIMAL，注意小数点scale范围不够是会截断四舍五入的，金钱类型到分，毫就已经完全够了，DECIMAL(18, 3)，就是百兆，可精确到毫，一般而言都够用了 浮点型MySQL uses four bytes for single-precision values and eight bytes for double-precision values. FLOAT(M,D) - 不能使用无符号的浮点数字。可以定义显示长度(M)和小数位数(D)。这不是必需的，并且默认为10,2。其中2是小数的位数，10是数字(包括小数)的总数。小数精度可以到24个浮点。 *DOUBLE(M,D) *- 不能使用无符号的双精度浮点数。可以定义显示长度(M)和小数位数(D)。 这不是必需的，默认为16,4，其中4是小数的位数。小数精度可以达到53位的DOUBLE。 REAL是DOUBLE同义词。 For example, a column defined as FLOAT(7,4) will look like -999.9999 when displayed. MySQL performs rounding when storing values, so if you insert 999.00009 into a FLOAT(7,4) column, the approximate result is 999.0001. 日期类型 Data Type “Zero” Value Storage (Bytes) Time Range DATE 0000-00-00 3 1000-01-01 to 9999-12-31 TIME 00:00:00 -838:59:59 to 838:59:59 DATETIME 0000-00-00 00:00:00 8 1000-01-01 00:00:00 to 9999-12-31 23:59:59 TIMESTAMP 0000-00-00 00:00:00 4 1970-01-01 00:00:01 UTC to 2038-01-19 03:14:07 UTC YEAR 0000 1 1901 to 2155 现在都2020年了，不晓得TIMESTAMP以后咋整，然后TIMESTAMP的特点是有时区特性 DATE 具体到天，DATETIME/ TIMESTAMP/ TIME 可具体到6位小数点（微秒） 字符串类型 Value CHAR(4) Storage Required VARCHAR(4) Storage Required ‘’ ‘ ‘ 4 bytes ‘’ 1 byte ‘ab’ ‘ab ‘ 4 bytes ‘ab’ 3 bytes ‘abcd’ ‘abcd’ 4 bytes ‘abcd’ 5 bytes ‘abcdefgh’ ‘abcd’ 4 bytes ‘abcd’ 5 bytes CHAR类型定长会使用空格进行填充 VARCHAR类型不定长 其他字符类存储的是二进制 Tip 开启严格的sql_mode，避免一些非法插入 字符集选用utf8mb4，utf8是它的子集，扩充了一些移动端的字符","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://github.com/linlinnn/tags/MySQL/"}]},{"title":"MySQL命令","slug":"MySQL 命令","date":"2020-04-11T15:08:26.335Z","updated":"2020-04-11T15:10:53.942Z","comments":true,"path":"2020/04/11/MySQL 命令/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/MySQL%20%E5%91%BD%E4%BB%A4/","excerpt":"","text":"MySQL命令用户权限创建/删除账号12create user &#39;app&#39;@&#39;127.0.0.1&#39;;drop user &#39;app&#39;@&#39;127.0.0.1&#39;; 授予/回收权限12grant select on user.table1 to &#39;app&#39;@&#39;127.0.0.1&#39;;revoke select on user.table1 to &#39;app&#39;@&#39;127.0.0.1&#39;; 代理（实现组管理）12grant proxy on &#39;dba&#39;@&#39;127.0.0.1&#39; to &#39;app&#39;@&#39;127.0.0.1&#39;;grant select on user.table1 to &#39;dba&#39;@&#39;127.0.0.1&#39;; 查看权限1show grants for &#39;dba&#39;@&#39;127.0.0.1&#39;; 慢查询日志参数说明 参数 说明 版本 slow_query_log 是否开启慢查询日志 slow_query_log_file 慢查询日志文件名 long_query_time 慢查询阈值（大于） 5.5 支持毫秒 min_examined_row_limit 扫描记录少于该值的SQL不记录到慢查询日志 log_queries_not_using_indexes 没有使用索引的SQL记录 log_throttle_queries_not_using_indexes 限制每分钟记录没有使用索引的次数 5.6 log_slow_admin_statements 记录管理操作 log_output 慢查询日志格式（FILE|TABLE|NONE） 5.5 log_slow_slave_statements 从服务器上开启慢查询日志 log_timestamps 写入时区信息 5.7 参考配置123456789slow_query_log = 1slow_query_log_file = slow.loglog_queries_not_using_indexes = 1log_slow_admin_statements = 1log_slow_slave_statements = 1log_throttle_queries_not_using_indexes = 10expire_logs_days = 90long_query_time = 2min_examined_row_limit = 100 格式化慢查询日志1mysqldumpslow slow.log 如果慢查询日志文件比较大的话，可以截取出其中的一部分，如tail -10000f slow.log &gt; slow10000.log 因为一般来说这段时间的慢查询日志都是由于类似的查询语句产生的 也可以将慢查询日志存放到TABLE，mysql.slow_log，默认的存储引擎是CSV，先关闭改成myisam再开启 set global slow_query_log = 0; alter table slow_log engine = myisam; set global slow_query_log = 1; Tip查看frm文件12# 安装mysql-utilitiesmysqlfrm --diagnostic xxx.frm","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://github.com/linlinnn/tags/MySQL/"}]},{"title":"MySQL 存储引擎","slug":"MySQL 存储引擎","date":"2020-04-11T15:08:26.305Z","updated":"2020-04-11T15:10:02.321Z","comments":true,"path":"2020/04/11/MySQL 存储引擎/","link":"","permalink":"https://github.com/linlinnn/2020/04/11/MySQL%20%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/","excerpt":"","text":"MySQL 存储引擎版本：5.7.11 存储引擎的类别1select engines; 存储引擎/engines.JPG MyISAMMySQL5.1版本之前（包括5.1）的默认存储引擎 堆表数据结构 表锁设计 支持数据静态压缩 不支持事务 数据容易丢失 索引容易损坏（不保证数据文件和索引文件同步更新，写入到操作系统缓存，如果发生宕机或者磁盘空间满了） 唯一的优点：数据文件可以直接拷贝到另一台服务器使用 CSV以MySQL标准接口访问CSV文件 CSV所有的列都是NOT NULL Federated允许本地访问远程MySQL数据库中表的数据 本地不存储任何数据文件 默认是关闭的","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://github.com/linlinnn/tags/MySQL/"}]},{"title":"深入JVM之java内存模型","slug":"深入JVM之java内存模型","date":"2020-03-08T21:05:43.291Z","updated":"2020-03-08T21:08:27.297Z","comments":true,"path":"2020/03/09/深入JVM之java内存模型/","link":"","permalink":"https://github.com/linlinnn/2020/03/09/%E6%B7%B1%E5%85%A5JVM%E4%B9%8Bjava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Java内存模型基本概念​ Java内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到内存和从内存中取出变量值的底层细节。 ​ Java内存模型规定所有的变量存储在主内存中。每条线程都有自己的工作内存，线程的工作内存中保存了被该线程使用的变量的主内存副本（不是复制整个对象，如对象的引用，对象中某个被线程访问到的字段），线程对变量的所有操作都必须在工作内存中进行，而不能直接读取主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。 内存间交互操作​ 关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存，如何从工作内存同步回主内存的实现细节，Java内存模型中定义了以下8种操作来完成。Java虚拟机实现时必须保证这些操作是原子的，不可再分的。（double和long类型的变量有些特别，允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行） lock：作用于主内存的变量，它把一个变量标识为一条线程独占的状态 unlock：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中 load：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中 use：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值得字节码指令时执行这个操作 assign：作用于工作内存的变量，它把一个从执行引擎接受的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作 store：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中 write：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中 Java内存模型规定了在执行上述操作时必须满足如下规则： 不允许read和load，store和write操作之一单独出现 变量在工作内存中改变了之后必须把该变化同步回主内存 不允许一个线程没有发生任何的assign操作就把工作内存同步回主内存 一个新的变量只能从主内存中“诞生” 一个变量同一时刻只能被一个线程lock，但同一个线程可以lock多次，然后执行相同次数的unlock才能释放 对一个变量执行lock，将清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值 一个变量没有lock就不能unlock，也不允许去unlock一个被其他线程锁定的变量 对一个变量unlock前，必须先把此变量同步回主内存中 volatile当一个变量被定义成volatile之后，它将具备两项特性： 保证此变量对所有线程的可见性，“可行性”指的是当一个线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的，由于volatile只能保证可见性，在不符合以下两条规则的运算场景中，仍然要通过加锁来保证原子性 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值 变量不需要与其他的状态变量共同参与不变约束 禁止指令重排序优化，普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的记过，而不能保证变量赋值操作的顺序与程序代码的执行顺序一致。 先行发生原则（Happens-Before） 程序次序规则：在一个线程内，按照控制流顺序，前面的操作先行发生于后面的操作（区分时间先后顺序） 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作 volatile规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作 线程启动规则：Thread的start先行发生于此线程的所有动作 线程终止规则：线程的所有操作都先行发生于此线程的终止检测 线程中断规则：对线程interrupt方法的调用先行发生于被中断线程的代码检测到中断事件的发生 对象终结规则：一个对象的初始化完成先行发生于它的finalize方法的开始 传递性：A先于B，B先于C，则A先于C","categories":[],"tags":[]},{"title":"深入JVM之java线程基础","slug":"深入JVM之java线程基础","date":"2020-03-08T13:30:12.782Z","updated":"2020-03-08T13:33:36.277Z","comments":true,"path":"2020/03/08/深入JVM之java线程基础/","link":"","permalink":"https://github.com/linlinnn/2020/03/08/%E6%B7%B1%E5%85%A5JVM%E4%B9%8Bjava%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Java线程Java线程状态 新建（New）：创建后未启动的线程 运行（Runnable）：正在执行或者正在等待操作系统为它分配执行时间 无限期等待（Waiting）：等待被其他线程显示唤醒 Object::wait() Thread::join() LockSupport::park() 限期等待（Timed Waiting）：一定时间后由系统自动唤醒 Thread::sleep() Object::wait(Timeout) Thread::join(TImeout) LockSupport::parkNanos() LockSupport::parkUntil() 阻塞（Blocked）：等待获取排它锁 结束（Terminated）：线程已经结束执行 内核线程的调度成本主要来自于用户态与核心态的状态转换，而这两种状态转换的开销主要来自于响应中断、保护和恢复执行现场的成本 线程安全互斥同步​ synchronized持有锁是一个重量级的操作，Java的线程是映射到操作系统的原生内核线程之上的，如果要阻塞或唤醒一条线程，则需要操作系统帮忙完成，这就不可避免地陷入用户态到核心的转换，进行这种状态转换需要耗费很多的处理器时间 ​ 除了synchronized关键字外，自JDK5，基于Lock接口，用户能够以非块结构来实现互斥同步 ​ 重入锁ReentrantLock 是Lock接口的一种实现，它与synchronized很相似，相比增加了一些高级功能，主要由三项：等待可中断、可实现公平锁及锁可以绑定多个条件： 等待可中断：当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。可中断特性对处理执行时间非常长的同步块很有帮助 公平锁：多个线程等待同一个锁时，必须按照申请锁的时间顺序来依次获取锁，而非公平锁不保证这一点，使用非公平锁，将会导致ReentrantLock 性能急剧下降，影响吞吐量 锁绑定多个条件：可以同时绑定多个Condition 对象 synchronized和ReentrantLock 都能够满足需求时，推荐使用synchronized 同步地语义更加清晰 Lock需要对应finally-unlock，但是synchronized会保证释放锁 Lock和synchronized目前性能差不多，不过synchronized将来有更多的优化空间 非阻塞同步​ 互斥同步是一种悲观的并发策略（实际上虚拟机会优化掉很大一部分不必要的加锁），另一个选择是基于冲突检测的乐观并发策略，乐观并发策略需要“硬件指令集的发展”，因为要求操作和冲突检测这两个步骤具备原子性，这类指令常用的有： 测试并设置（Test-and-Set） 获取并增加（Fetch-and-Increment） 交换（Swap） 比较并交换（Compare-and-Swap） 加载链接/条件储存（Load-Linked/Store-Conditonal） CAS指令需要三个操作数，分别是内存位置、旧的预期值、准备设置的新值 12345678910// Unsafe.class// unsafe.getAndAddInt(this, valueOffset, 1) + 1public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 无限循环中，不断尝试将一个比当前值大一的新值赋值给自己，如果失败了，则说明旧值已经发生变化 ABA问题，可以通过一个带有标记原子引用类AtomicStampedReference， 大部分情况下ABA问题不会影响程序并发的正确性，如果要解决ABA问题，改为互斥同步可能会比原子类更为高效 无同步方案线程本地存储（Thread Local Storage），每个线程持有一份变量副本，消除竞争关系 锁优化自旋锁与自适应自旋​ 现在的物理机器普通都是多核处理器系统，能够让两个或以上的能够同时并行执行，可以让后面请求锁的那个线程“稍等一会”，看看持有锁的线程是否很快就会释放锁。为了让线程等待，只需让线程执行一个忙循环（自旋），这项技术就是自旋锁 ​ 自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，所以如果锁被占用的时间很短，自旋等待的效果就好，反之则只会白白消耗处理器资源。因此自旋等待必须有一定的时间限制，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式挂起线程。默认值是十次，-XX:PreBlockSpin设置 ​ 不过无论是默认值还是用户指定的自旋次数，对整个Java虚拟机中所有的锁来说都是相同的。在JDK6中对自旋锁的优化，引入了自适应的自旋，由前一次在同一个锁的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在进行中，那么虚拟机就会认为这个自旋也很有可能再次成功，进而允许自旋等待持续更长的时间，比如持续100次忙循环。另一方面，如果对于某个锁，自旋很少成功获得过锁，那么以后就有可能忽略掉自旋的过程，直接挂起 锁消除​ 锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除，主要依据是逃逸分析的数据支持。 锁粗化​ 虚拟机检测到一串操作都对同一对象的加锁，放大加锁的范围到整个操作序列的外部 轻量级锁​ 在没有多线程竞争的前提下（大部分的锁，在整个同步周期内都不存在竞争），减少传统的重量级锁使用操作系统互斥量产生的性能消耗 偏向锁​ 消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能，即对比轻量级锁连CAS操作都省去","categories":[],"tags":[{"name":"深入JVM","slug":"深入JVM","permalink":"https://github.com/linlinnn/tags/%E6%B7%B1%E5%85%A5JVM/"}]},{"title":"网络通信之TCP","slug":"网络通信之TCP","date":"2020-03-08T13:20:52.339Z","updated":"2020-03-08T13:29:06.185Z","comments":true,"path":"2020/03/08/网络通信之TCP/","link":"","permalink":"https://github.com/linlinnn/2020/03/08/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E4%B9%8BTCP/","excerpt":"","text":"网络通信之TCP1.1、简介： 面向连接的、可靠的、基于字节流的传输层通信协议 将应用层的数据流分割成报文段并发送给目标节点的TCP层，数据包都有序号，对方收到则发送ACK确认，保证有序接收、重复报文自动废弃、未收到则重传 使用奇偶校验和来检验数据在传输过程中是否有误 双向传递（全双工） 流量缓冲：解决速度不匹配问题 拥塞控制 1.2、TCP报文 TCP Flags ACK: 确认序号标志 SYN: 请求连接标志 FIN: 释放连接标志 PSH:数据传输标志 1.3、三次握手 1.3.1、SYN报文 第一次握手：SYN=1，随机生成seq序号989512128（） Client 状态：CLOSED =&gt; SYN_SENT Server状态：LISTEN =&gt; SYN_REVD 1.3.2、SYN/ACK报文 第二次握手：SYN=1，ACK=1, ack=seq+1=989512129, 随机生成seq序号753743756 Client 状态：SYN_SENT =&gt; ESTALISHED 1.3.3、ACK报文 第三次握手：ACK=1，ack=seq+1=753743757 Server状态：SYN_REVD =&gt; ESTABLISHED，成功建立连接，可以开始数据传输 为什么两次握手不可以？ 假设是两次握手，客户端发送了SYN报文1，但没有收到SYN/ACK报文，以为报文丢失，重新发送SYN报文2，此时连接建立然后关闭，但SYN报文1 此时到达了服务端，再次建立连接，导致错误连接和资源浪费。 而三次握手即使服务端收到了失效的SYN报文1 ，由于客户端不会发送ACK报文，此时服务端收不到ACK报文，就不会建立连接 1.3.4、PSH/ACK报文 ACK=1，PSH=1，开始传输数据，数据长度为285 1.3.5、ACK报文 ACK=1，ack=seq+len=989512129+285=989512414 1.4、SYN攻击​ 攻击者短时间伪造不同IP地址的SYN报文，快速占满backlog队列，服务器端的连接都处于SYN_RECEIVED的状态，占用大量的资源，使得正常的连接无法建立。 net.core.netdev_max_backlog 接受自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn SYN_RCVD状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的SYN直接回包RST，丢弃连接 net.ipv4.tcp_syncookies = 1 当SYN队列满后，新的SYN不进入队列，计算出cookie再以SYN+ACK的序列号返回客户端，正常客户端发报文时，服务器根据报文中携带的cookie重新恢复连接 由于cookie占用序列号空间，导致此时所有TCP可选功能失效 1.4、三次握手优化1.4.1、操作系统内核限制调整 在操作系统内核中分别用SYN队列和ACCEPT队列来维护相应的连接套接字 当连接数非常多的时候可以对操作系统内核限制进行调整 服务器端SYN_RCV状态 net.ipv4.tcp_max_syn_backlog：SYN_RCVD状态连接的最大个数 net.ipv4.tcp_synack_retries：被动建立连接时，发SYN/ACK的重试次数 客户端SYN_SENT状态 net.ipv4.tcp_syn_retries = 6 主动建立连接时，发SYN的重试次数 net.ipv4.ip_local_port_range = 32768 ~ 60999 建立连接时的本地端口可用范围 ACCEPT队列设置 TCP_DEFER_ACCEPT：当有数据报文时操作系统内核才激活应用程序 1.4.2、Fast Open 降低时延 每一次TCP连接数据传输都需要2*RTT时间 TCP协议提供了Fast Open方式，服务器将第一次建立连接成功的相关信息保存在Cookie缓存在客户端中，客户端后续请求建立连接通过携带这个Cookie消除了三次握手，使得只需要1*RTT的时间，若Cookie丢失，则在RTO后发起普通的三次握手连接 Server校验Cookie（解密Cookie以及比对IP地址或者重新加密IP地址以和接收到的Cookie进行对比）。 如果验证成功，向用户发送SYN+ACK，在用户回复ACK之前，便可以向用户传输数据； 如果验证失败，则丢弃此TFO请求携带的数据，回复SYN-ACK确认SYN Seq，完成正常的三次握手。 建立了TFO连接而又没有完成TCP连接的请求在Server端被称为pending TFO connection，当pending的连接超过上限值，Server会关闭TFO，后续的请求会按正常的三次握手处理。 如果一个带有TFO的SYN请求如果在一段时间内没有收到回应，用户会重新发送一个标准的SYN请求，不带 任何其他数据。 Linux上打开TCP Fast Open net.ipv4.tcp_fastopen：系统开启TFP功能 0： 关闭 1：作为客户端时可以使用TFO 2：作为服务器时可以使用TFO 3：无论作为客户端还是服务器，都可以使用TFO 1.5、四次挥手 为什么建立连接时三次握手，释放连接却需要四次挥手？ 客户端发送FIN报文表示客户端不再发送数据（FIN-WAIT-1状态），但可以接收数据，服务端也还可以发送数据 服务端发送ACK报文，进入FIN-WAIT-2状态，服务端进入CLOSE-WAIT状态 服务端将最后的数据发送完毕，发送FIN报文，进入LAST-ACK状态 客户端发送ACK报文， 进入TIME-WAIT状态，此时TCP连接还没有释放，必须经过2*MSL时间后，当客户端撤销相应的TCB后，才进入CLOSED状态 服务端只要受到客户端发出的确认，立即进入CLOSED状态，撤销TCB，结束TCP连接 为什么客户端发送ACK报文后要等待2*MSL（Maximum Segment Lifetime）时间？ 保证客户端发送的ACK报文可以到达服务端，假设此报文丢失，服务端就会认为客户端没有接收到FIN报文，进行重发，客户端就可以在2MSL时间内收到重发的报文，发送ACK报文，重新等待2MSL时间 在2*MSL时间内产生的报文都会在网络中消失，防止新连接接收到就连接的请求报文 大量的连接处于TIME_WAIT状态，导致部分客户端连接不上？ 主要关注场景：服务端主动关闭连接，高并发短连接 /etc/sysctl.conf -p让参数生效 /etc/sysctl.conf是一个允许改变正在运行中的Linux系统的接口，它包含一些TCP/IP堆栈和虚拟内存系统的高级选项，修改内核参数永久生效。 net.ipv4.tcp_tw_reuse = 1 表示开启重用。（客户端）允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 由于timestamp的存在，操作系统可以拒绝迟到的报文 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_tw_recycle = 1 （客户端和服务端）表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 不安全，无法避免报文延迟、重复等给新连接造成混乱 net.ipv4.tcp_fin_timeout 修改默认的 TIMEOUT 时间 net.ipv4.tcp_keepalive_time = 1200 表示当keepalive起用的时候，TCP发送keepalive消息的频度，缺省是2小时，改为20分钟 net.ipv4.ip_local_port_range = 1024 65000 表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000 net.ipv4.tcp_max_syn_backlog = 8192 表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数 net.ipv4.tcp_max_tw_buckets = 5000 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息 默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于 Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死 大量连接处于CLOSE_WAIT状态 主要关注场景：一般是程序没有正常地释放资源导致 分析思路： 出现问题后，立马应该检查日志，确实日志没有发现问题； 监控明确显示了socket不断增长，很明确立马应该使用 netstat 检查情况看看是哪个进程的锅； 根据 netstat 的检查，使用 tcpdump 抓包分析一下为什么连接会被动断开； 如果熟悉代码应该直接去检查业务代码，如果不熟悉则可以使用 perf 把代码的调用链路打印出来； 不论是分析代码还是火焰图，到此应该能够很快定位到问题。 SO_REUSEADDR可以用在以下四种情况下。 (摘自《Unix网络编程》卷一) 1、当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你启动的程序的socket2要占用该地址和端口，你的程序就要用到该选项。 2、SO_REUSEADDR允许同一port上启动同一服务器的多个实例(多个进程)。但每个实例绑定的IP地址是不能相同的。在有多块网卡或用IP Alias技术的机器可以测试这种情况。 3、SO_REUSEADDR允许单个进程绑定相同的端口到多个socket上，但每个socket绑定的ip地址不同。这和2很相似，区别请看UNPv1。 4、SO_REUSEADDR允许完全相同的地址和端口的重复绑定。但这只用于UDP的多播，不用于TCP。 1.6、滑动窗口（流量控制）1、通过一个最新ack序列号来确认在这之间的报文都以确认接收，丢包只需重发可发送窗口内ack序列号之后的报文。 2、接收方根据自己的处理情况返回下次发送多少个报文来控制滑动窗口大小，为避免该报文丢失，发送方会定时发送一个窗口探测报文。 1.7、netstat命令查看TCP状态1234567netstat -a 显示所有连接和监听端口-n 以数字形式（如IP地址）显示地址和端口号-r 显示路由表-s 显示每个协议的统计信息-o(Windows) 显示拥有的与每个连接关联的进程ID-b(Windows)/-p(Linux) 显示对应的可执行程序的名字","categories":[],"tags":[{"name":"网络通信","slug":"网络通信","permalink":"https://github.com/linlinnn/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"}]},{"title":"代码精进之路——读书笔记（1）","slug":"代码精进之路——读书笔记（1）","date":"2020-03-08T04:00:07.520Z","updated":"2020-03-08T11:08:48.703Z","comments":true,"path":"2020/03/08/代码精进之路——读书笔记（1）/","link":"","permalink":"https://github.com/linlinnn/2020/03/08/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/","excerpt":"","text":"代码精进之路——读书笔记（1）1、前言​ 很多时候看到自己写代码或是别人写的代码就觉得要吐了，可是又很困惑该如何改进，想起一首rap，当我总是被这些问题给缠住，慢慢地想要克服它变成一种难度，周末的时候买了本新书《代码精进之路》，原来这是很多程序员都会遇到的情况，软件的复杂性是一个基本特征，再加上源源不断的需求压力总是使开发人员妥协，引发破窗效应，系统越来越混乱。故此记录下一些很有启发的理念。 2、命名规范2.1、代码注释​ 如果注释是为了阐述代码背后的意图，那么这个注释是有意义的； ​ 如果注释是为了复述代码功能，那么有可能就意味着代码的坏味道； ​ 好的代码通过好的命名规范使得自身意图是显性化的，就如同看一篇说明文。 123456789101112// 复述代码功能：线程休眠2秒// 阐述代码背后的意图：线程休眠2秒，为了等待相关系统处理结果Thread.sleep(2000);// 代码自释：不需要注释也能清楚获悉代码本意private void waitProcessResultFromA()&#123; try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; LOGGER.error(e); &#125;&#125; 3、函数3.1、单一职责法则​ SRP（Single Responsibility Principle）单一职责法则，现阶段自身的系统抽象能力不足，还需要很多的经验和总结，但是将这一法则作用于函数是每个程序员都能够做到的，而将函数写好了，系统也会开始往好的方向衍进。 ​ 一个函数由命名、入参、出参组成，只要明确这三个点的意义，函数实现起来也会变得清晰，拿leetcode上一道简单题打个比方。 ​ 一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 ​ 以前遇到这种问题总是会陷入一种难以言喻的递归模拟，但其实函数的职责是很明确的，返回n级台阶的跳法 总数，入参为台阶级数，递归边界就是1级台阶只有1种跳法，2级台阶有两种跳法。 123456// 函数意义：台阶级数为floor层的跳法private void jumpWays(int floor)&#123; if(floor &lt;= 2) return floor; // 翻译一下递归就是，跳一级，也可以跳两级，返回总共的跳法 return jumpWays(floor - 1) + jumpWays(floor - 2);&#125; ​ 当然还要对递归子问题使用记忆化进行优化。 ​ 这样就有一种感觉，写代码就是将需求以合适的方式复述出来，是一篇说明文。 3.2、优化判空​ NPE(Null Pointer Exception)是需要时刻注意判断的，这就让要获取一些属性链路比较长的属性时经常写出这种令人呕吐的代码，可以使用Java 8的新特性Optional 进行优化 12345678910111213141516// 为了获取user里的isocodeif(user != null)&#123; Address address = user.getAddress(); if(address != null)&#123; Country country = address.getCountry(); if(country != null)&#123; String isocode = country.getIsocode(); &#125; &#125;&#125;// 比起大量的if语句，这样的链式调用更符合思维String isocode = Optional.ofNullable(user) .flatMap(User::getAddress) .flatMap(Address::getCountry) .map(Country::getIsocode) .orElse(\"default\"); 3.3、组合函数模式​ 组合函数有助于代码保持精炼并易于复用，这样的代码就像一本很多说明文组成的书，入口函数是目录，目录的内容指向具体的私有函数。 ​ Spring中BeanUtils#copyProperties 方法有57行，太长的函数容易让人迷失陷入细节，增大理解难度。当然细节是很重要的，只不过一次只关注一个点更利于提高效率，把握住重点，经过分析，这个函数主要做了两件事：一、判断能不能copy，二、执行copy。因此入口函数拆分为两个步骤。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public void copyProperties(Object dest, Object orig) throws IllegalAccessException, InvocationTargetException &#123; // 1、checkDestAndOrig(dest, orig); 判断能不能copy // ========================================================= // Validate existence of the specified beans if (dest == null) &#123; throw new IllegalArgumentException (\"No destination bean specified\"); &#125; if (orig == null) &#123; throw new IllegalArgumentException(\"No origin bean specified\"); &#125; if (log.isDebugEnabled()) &#123; log.debug(\"BeanUtils.copyProperties(\" + dest + \", \" + orig + \")\"); &#125; // 2、copyOrigToDest(orig, dest); 执行copy，这段逻辑还是稍微长了，可以继续划分 // ========================================================= // Copy the properties, converting as necessary if (orig instanceof DynaBean) &#123; DynaProperty origDescriptors[] = ((DynaBean) orig).getDynaClass().getDynaProperties(); for (int i = 0; i &lt; origDescriptors.length; i++) &#123; String name = origDescriptors[i].getName(); if (getPropertyUtils().isWriteable(dest, name)) &#123; Object value = ((DynaBean) orig).get(name); copyProperty(dest, name, value); &#125; &#125; &#125; else if (orig instanceof Map) &#123; Iterator names = ((Map) orig).keySet().iterator(); while (names.hasNext()) &#123; String name = (String) names.next(); if (getPropertyUtils().isWriteable(dest, name)) &#123; Object value = ((Map) orig).get(name); copyProperty(dest, name, value); &#125; &#125; &#125; else /* if (orig is a standard JavaBean) */ &#123; PropertyDescriptor origDescriptors[] = getPropertyUtils().getPropertyDescriptors(orig); for (int i = 0; i &lt; origDescriptors.length; i++) &#123; String name = origDescriptors[i].getName(); if (\"class\".equals(name)) &#123; continue; // No point in trying to set an object's class &#125; if (getPropertyUtils().isReadable(orig, name) &amp;&amp; getPropertyUtils().isWriteable(dest, name)) &#123; try &#123; Object value = getPropertyUtils().getSimpleProperty(orig, name); copyProperty(dest, name, value); &#125; catch (NoSuchMethodException e) &#123; ; // Should not happen &#125; &#125; &#125; &#125;&#125; ​ 很多时候只要多做一点点，就可以写出更好的代码。","categories":[],"tags":[{"name":"代码精进之路","slug":"代码精进之路","permalink":"https://github.com/linlinnn/tags/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF/"}]},{"title":"Java源码之ConcurrentHashMap","slug":"Java源码之ConcurrentHashMap","date":"2020-02-27T14:49:23.769Z","updated":"2020-03-08T04:24:53.727Z","comments":true,"path":"2020/02/27/Java源码之ConcurrentHashMap/","link":"","permalink":"https://github.com/linlinnn/2020/02/27/Java%E6%BA%90%E7%A0%81%E4%B9%8BConcurrentHashMap/","excerpt":"","text":"Java源码之ConcurrentHashMap1、带着问题看源码Q1：ConcurrentHashMap为什么是线程安全的？ Q2：JDK1.7和1.8的ConcurrentHashMap有什么不同？ Q3：动态扩容策略是什么，如何多线程扩容的？ 2、数据存储结构先看JDK1.8的，数组 + 链表 + 红黑树，为了保证扩容时的线程安全，还增加了一种转移节点 123456789101112131415161718192021222324// 扩容转移时的最小数组分组大小private static final int MIN_TRANSFER_STRIDE = 16;static final int MOVED = -1; // hash for 转移节点static final int TREEBIN = -2; // hash for 红黑树根节点static final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash// CPU核数static final int NCPU = Runtime.getRuntime().availableProcessors();// volatile 修饰哈希表，具有线程可见性transient volatile Node&lt;K,V&gt;[] table;// 扩容哈希表private transient volatile Node&lt;K,V&gt;[] nextTable;/** * Table initialization and resizing control. * -1: 初始化 * -(1 + 正在进行resize的线程数)，与-1区别开 * 0: 默认状态 * 在初始化之后，该值表示下一次扩容阈值 */private transient volatile int sizeCtl;// 转移下标private transient volatile int transferIndex;// 转移节点static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123;&#125; 3、初始化自旋 + cas + 双重check 1234567891011121314151617181920212223242526272829//初始化 table，通过对 sizeCtl 的变量赋值来保证数组只能被初始化一次private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; // 通过自旋保证初始化成功 while ((tab = table) == null || tab.length == 0) &#123; // 小于0代表有线程正在初始化，释放当前CPU的调度权 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 赋值保证当前只有一个线程在初始化，-1 代表当前只有一个线程能初始化 // 保证了数组的初始化的安全性 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; // 可能执行到这里的时候，table已经不为空了，这里是双重check if ((tab = table) == null || tab.length == 0) &#123; // 进行初始化 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); // 负载因子0.75 &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 3、查询元素不需要加锁，因为哈希表的节点用volatile修饰了，多线程之间具有可见性 12345678910111213141516171819202122232425public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 计算hashcode int h = spread(key.hashCode()); // 不是空的数组 &amp;&amp; 并且当前索引的槽点数据不是空的 // 否则该key对应的值不存在，返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 槽点第一个值和key相等，直接返回 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 如果是红黑树或者转移节点，使用对应的find方法 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 如果是链表，遍历查找 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 4、添加元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // HashMap时可以put null的，ConcurrentHashMap不可以 if (key == null || value == null) throw new NullPointerException(); // h ^ (h &gt;&gt;&gt; 16) &amp; HASH_BITS(0x7fffffff)，多&amp;了HASHBITS // hash的负在ConcurrentHashMap中有特殊意义表示在扩容或者是树节点 int hash = spread(key.hashCode()); int binCount = 0; // 当对应的哈希槽为转移节点时陷入自旋，等待扩容完成 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 哈希表为空，初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 如果当前索引位置为空，直接插入 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //cas在位置i创建新的元素，当位置i为空时，即能创建成功，结束自旋 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 如果当前哈希槽是转移节点，表示该槽点正在扩容，就会一直等待扩容完成 // 转移节点的hash值为MOVED=-1 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); // 哈希槽上有值，即发生冲突 else &#123; V oldVal = null; // 锁定当前哈希槽，其余线程不能操作，保证了安全 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; // 链表 if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // key已存在 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 记录旧值 oldVal = e.val; // 覆盖值 if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 把新增的元素添加到链表的末尾 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 红黑树，这里没有使用TreeNode,使用的是TreeBin，TreeNode只是红黑树的一个节点 // TreeBin持有红黑树的引用，并且会对其加锁，保证其操作的线程安全 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; // 在putTreeVal方法里面，在给红黑树重新着色旋转的时候，会锁住红黑树的根节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; // 记录旧值 oldVal = p.val; // 覆盖值 if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // binCount不为0 if (binCount != 0) &#123; // 链表是否需要转化成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); // 旧值不为空，返回旧值 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 哈希槽数 +1 addCount(1L, binCount); return null;&#125; 5、动态扩容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240//新增元素时，也就是在调用 putVal 方法后，为了通用，增加了check入参，用于指定是否可能会出现扩容的情况//check &gt;= 0 即为可能出现扩容的情况，例如 putVal方法中的调用private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // 类似LongAdder的计数 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // 检查当前哈希表元素个数s是否达到扩容阈值sizeCtl ，扩容时sizeCtl为负数，依旧成立，同时还得满足数组非空且数组长度不能大于允许的数组最大长度这两个条件 // 这个while循环除了判断是否达到阈值从而进行扩容操作之外还有一个作用就是当一条线程完成自己的迁移任务后，如果集合还在扩容，则会继续循环，帮助扩容，申请后面的迁移任务 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; // 根据length得到一个标识 int rs = resizeStamp(n); // sc &lt; 0说明集合正在扩容当中 if (sc &lt; 0) &#123; // 如果 sc 的高 16 位不等于标识符（说明sizeCtl变化了） // 如果 sc == 标识符 + 1（当一个扩容线程结束，就会将sc减一，全部扩容线程结束，sc就等于rs+1 ） // 如果 sc == 标识符 + MAX_RESIZERS（65535，即低16全为1，扩容线程数已经达到最大） // 如果 nextTable == null（结束扩容了） // 如果 transferIndex &lt;= 0 (转移状态变化了) // 结束循环 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 扩容还未结束，并且允许扩容线程加入，扩容线程数+1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 如果不在扩容，将sc更新：标识符左移16位 然后+2. 也就是变成一个负数。高16位是标识符，低16位初始是2（即为一个线程），后面扩容时会根据线程是否为这个值来确定是否为最后一个线程 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 更新sizeCtl为负数后，开始扩容 transfer(tab, null); s = sumCount(); &#125; &#125;&#125;// 扩容状态下其他线程对集合进行插入、修改、删除、合并、compute等操作时遇到 ForwardingNode 节点会调用该帮助扩容方法final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); //此处的 while 循环是上面 addCount 方法的部分逻辑 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125;// tab：原数组，nextTab：新数组private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; // 原数组的长度 int n = tab.length, stride; // 将 length / 8 然后除以 CPU核心数。如果得到的结果小于MIN_TRANSFER_STRIDE（16），那么就等于16。 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果新数组为空，初始化 if (nextTab == null) &#123; // initiating try &#123; // 扩容两倍 @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME // 扩容失败， sizeCtl设为INT_MAX sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; // transferIndex 表示转移时的下标边界 transferIndex = n; &#125; // 新数组的长度 int nextn = nextTab.length; // 代表转移节点，如果原数组上是转移节点，说明该节点正在被扩容 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 推进标识 boolean advance = true; // 标识是否完成 boolean finishing = false; // to ensure sweep before committing nextTab // 自旋，i 的值会从原数组划分的子区域的最大值开始，慢慢递减到 0 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; // 对 i 减一，判断是否大于等于 bound （正常情况下，如果大于 bound 不成立，说明该线程上次领取的任务已经完成了。那么，需要在下面继续领取任务） // 如果对 i 减一大于等于 bound（还需要继续做任务），或者完成了，修改推进状态为 false，不能推进了。任务成功后修改推进状态为 true。 // 通常，第一次进入循环，i-- 这个判断会无法通过，从而走下面的 nextIndex 赋值操作（获取最新的转移下标）。其余情况都是：如果可以推进，将 i 减一，然后修改成不可推进。如果 i 对应的桶处理成功了，改成可以推进。 if (--i &gt;= bound || finishing) advance = false; // 这里的目的是：1. 当一个线程进入时，会选取最新的转移下标。2. 当一个线程处理完自己的区间时，如果还有剩余区间的没有别的线程处理。再次获取区间。 else if ((nextIndex = transferIndex) &lt;= 0) &#123; // 如果小于等于0，说明没有区间了 ，i 改成 -1，推进状态变成 false，不再推进，表示，扩容结束了，当前线程可以退出了 i = -1; advance = false; &#125; // CAS 修改 transferIndex，即 length - 区间值，留下剩余的区间值供后面的线程使用 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; // 其中一个条件满足说明拷贝结束了 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; // 置空 table = nextTab; // 更新哈希表 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // 更新阈值 return; &#125; // 尝试将 sc -1. 表示这个线程结束扩容了，将 sc 的低 16 位减一。 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 如果sc - 2不等于标识符左移 16 位。如果相等了，说明没有线程在帮助扩容了,也就是说，扩容结束了。 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return;// 不相等，说明还有其他扩容线程，当前线程结束方法。 finishing = advance = true; // 相等，标记结束 i = n; // recheck before commit &#125; &#125; // 原哈希表位置i为空，用fwd转移节点占位 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 为转移节点，说明别的线程已经处理过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed // 到这里，说明这个位置有实际值了，且不是占位符。对这个节点加锁。加锁是为了防止 putVal 的时候向链表插入数据 else &#123; synchronized (f) &#123; // 判断 i 下标处的桶节点是否和 f 相同 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // low node, high node if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; // 记录上一个节点 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // 如果最后更新的runBit为0，设为low节点，为1则设为high节点 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; // 再次循环，生成两个链表，lastRun作为停止条件，为了避免多余的新建节点连接 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 在新数组相应位置上放置拷贝的值，原理和hashmap一样 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); // 在旧数组位置上放上 ForwardingNode 节点 // put 时，发现是 ForwardingNode 节点，就不会再动这个节点的数据了 setTabAt(tab, i, fwd); advance = true; &#125; // 红黑树的拷贝，同 HashMap 的内容 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; // 在旧数组位置上放上 ForwardingNode 节点 setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 看完之后觉得十分巧妙，引入一个转移节点，resize扩容的并发线程数与ConcurrentHashMap的操作频繁度相关，越频繁，并发扩容线程数越多，非常合理极致地利用CPU资源。 6、对比JDK1.76.1、数据存储结构数组（Segment） + 数组（HashEntry） + 链表（HashEntry节点） 12345// 默认并发度16static final int MAX_SEGMENTS = 1 &lt;&lt; 16;// 分段锁static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123;&#125;transient volatile HashEntry&lt;K,V&gt;[] table; 6.2、查询元素由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。 12345678910111213141516171819public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 计算hash值，对应具体的Segment int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; // 遍历元素，找到了就返回 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 6.3、添加元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); // 根据key找Segment int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false);&#125;final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); // put时锁定。如果当前没这条数据，则会返回新创建的HashEntry，否则为空 V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 返回数组中对应位置的元素（链表头部） // 遍历 for (HashEntry&lt;K,V&gt; e = first;;) &#123; // 节点不为空 if (e != null) &#123; K k; // key是否相等，是否进行覆盖 if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; // 如果数组对应位置为空 if (node != null) // 非空，则表示为新创建的值，头插法 node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); // 否则创建一个 int c = count + 1; if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 超过了容量阈值，但没达到最大限制，则扩容table else setEntryAt(tab, index, node); // 直接用新的node，替换掉旧的first node ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue;&#125;private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); // 返回与hash对应的数组内容 HashEntry&lt;K,V&gt; e = first;// 数组对应位置的链表头部 HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node while (!tryLock()) &#123;// 非阻塞获取锁 HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; // 如果头节点为空，创建新node，作为链表头部 if (node == null) // speculatively create node node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) // 存在，则比较是否为同一个key retries = 0; else // 存在，且链表头部和当前插入的值非同，则比较。 e = e.next; &#125; else if (++retries &gt; MAX_SCAN_RETRIES) &#123; // 最多tryLock次数，超过次数，就阻塞 lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 6.4、动态扩容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) &#123; // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot // 与1.8类似 HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; newTable[lastIdx] = lastRun; for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; // 相当于 i 或 i + n HashEntry&lt;K,V&gt; n = newTable[k]; // 头插法 newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 插入新节点 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 7、总结Q1：ConcurrentHashMap为什么是线程安全的？ Q2：JDK1.7和1.8的ConcurrentHashMap有什么不同？ 1、锁粒度和使用锁 JDK1.7的ConcurrentHashMap通过分段锁机制减小锁的粒度，使用可重入锁ReentrantLock保证线程安全 JDK1.8进一步缩小了锁的粒度至哈希槽，使用CAS机制和Synchronized（锁优化）保证线程安全 2、并行度控制 JDK1.7通过分段锁的个数控制并发，默认是16 JDK1.8通过转移节点，扩容时并行度可以根据容器操作的频率进行动态的调整 3、数据结构 ​ 和HashMap一样，JDK1.8增加了红黑树和链表的转换，主要是兜底哈希函数效果不好的时候，查询性能至少能维持在O(logn) Q3：动态扩容策略是什么，如何多线程扩容的？ ​ 扩容至原来的两倍，对原哈希表的数据进行重新散列 ​ 转移节点和sizeCtl标识十分重要，当一个哈希槽的节点为转移节点时，put操作的线程会进行等待或帮助扩容，每个线程领取哈希表中的一段进行转移，转移完成后领取剩余断，扩容结束后更新sizeCtl，从而根据put的频繁程度并行度也会相应地动态变化。","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之HashMap","slug":"Java源码之HashMap","date":"2020-02-26T07:51:41.426Z","updated":"2020-02-27T16:03:15.159Z","comments":true,"path":"2020/02/26/Java源码之HashMap/","link":"","permalink":"https://github.com/linlinnn/2020/02/26/Java%E6%BA%90%E7%A0%81%E4%B9%8BHashMap/","excerpt":"","text":"Java源码之HashMap1、带着问题看源码Q1：江湖规矩，请问HashMap是线程安全的吗？为什么线程不安全？ Q2：JDK1.7和1.8的HashMap有哪些区别？ Q3：如何解决Hash冲突解决？ Q4：动态扩容的策略是什么？ 2、数据存储结构先从1.8的看起，HashMap底层的数据结构是数组+链表+红黑树 显然最突出就是一个链表和红黑树的转换 12345678910111213141516171819202122// 默认的HashMap的空间大小16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// hashMap最大的空间大小static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// HashMap默认负载因子，负载因子越小，hash冲突机率越低，至于为什么，看完下面源码就知道了static final float DEFAULT_LOAD_FACTOR = 0.75f;// 链表长度大于等于8时，链表转化成红黑树static final int TREEIFY_THRESHOLD = 8;// 红黑树大小小于等于6时，红黑树转化成链表static final int UNTREEIFY_THRESHOLD = 6;// 当数组容量大于 64 时，链表才会转化成红黑树static final int MIN_TREEIFY_CAPACITY = 64;// 存放数据的数组transient Node&lt;K,V&gt;[] table;// 临界值（超过这个值则开始扩容）int threshold;// HashMap 负载因子final float loadFactor;// 链表的节点static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;// 红黑树的节点static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; 3、初始化有四种初始化方法，指定初始容量和负载因子，指定初始容量，无参，指定数据集合 12345678910111213141516171819202122232425262728293031323334353637public HashMap(int initialCapacity, float loadFactor) &#123; // 边界值判断 if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; // 根据初始容量计算扩容临界值，&gt;=initialCapacity的2的最小的幂次方 this.threshold = tableSizeFor(initialCapacity);&#125;// 注意这里没有32位的移动，因为最大容量MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; 4、查询元素1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; // tab:哈希表，first:hash对应哈希槽，n:哈希表的长度， Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 第一个元素是不是想要查询的？是的话直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 遍历查询 if ((e = first.next) != null) &#123; // 红黑树，BST查询 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 链表，遍历查询 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 知道元素时怎样查的了，那么添加元素肯定就要与之对应起来。 5、添加元素12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; // key为null返回0，反则hash码后与自身高16位进行异或 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; // tab:哈希表，n:哈希表的长度，i:散列后落到哈希表的索引，p：索引i对应的节点 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果哈希表为空，进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果节点为空，直接设为新节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 旧节点和新节点哈希值和key都相同 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 节点p是一个红黑树节点，按照红黑树的方式设置 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 是一个链表 for (int binCount = 0; ; ++binCount) &#123; // 遍历链表至末尾 if ((e = p.next) == null) &#123; // 新节点插入到链表尾部 p.next = newNode(hash, key, value, null); // 链表长度超过阈值8的话，转换成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 循环过程中遇到一个与新节点相同的节点，退出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // e不为空代表有一个节点哈希值和key都和新节点相同 if (e != null) &#123; // existing mapping for key V oldValue = e.value; // 如果不是putIfAbsent或者旧值为空，替换为新值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 哈希表超过扩容阈值，进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; ​ 红黑树是一种平衡二叉查找树，查询效率为O(logn) ​ 为什么大于等于8要进行链表到红黑树的转换，注释上大佬有说，因为正常情况下根据泊松分布，有8个哈希值冲突的概率是0.00000006，更多的话就小于千万分之一，一般根本不会发生，之后是小弟的认知，以此推论如果发生了的话就是散列函数出了问题或者是数据非常特殊导致不断发生哈希冲突，在这种情况下，红黑树作为一种查询性能较高的数据结构作为兜底，维持到O(logn) 6、动态扩容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // oldCapacity:旧哈希表的大小 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 扩容阈值 int oldThr = threshold; // newCapacity:新哈希表的大小，newThreshold:新的扩容阈值，大佬是不喜欢长命名么。。 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 旧哈希表大小 &gt;= 2^30，直接到顶INT_MAX if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 新哈希表大小 * 2 &lt; 2^30 且 旧哈希表大小 &gt;= 16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 扩容阈值 * 2 newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 旧哈希表大小为0，旧扩容阈值&gt;0，初始化为旧扩容阈值 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 旧哈希表大小为0，旧扩容阈值为0，初始化为默认值 newCap = DEFAULT_INITIAL_CAPACITY; //16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); // 16*0.75=12 &#125; // 新扩容阈值为0 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; // 容量*负载因子 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 将旧的哈希表元素重新散列到新的哈希表 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 该哈希槽只有一个元素 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 该哈希槽内对应的是红黑树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 该哈希槽内对应的是链表 else &#123; // preserve order // 位置不变的链表 Node&lt;K,V&gt; loHead = null, loTail = null; // 位置+oldCap的链表 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 为0则还在原位置 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 需要移动+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 位置不变的链表不为空，原位置指向头节点 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 位置+oldCap的链表不为空，原位置+oldCap指向头节点 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 扩容策略是当哈希表中的元素个数大于哈希表大小 * 负载因子，哈希表大小*2，乘2很好理解，因为保持了2的幂次方，可以看到扩容后需要对原来的元素进行重新散列，这个过程是非常消耗性能的。 负载因子默认为0.75，哈希表默认大小为16，当插入第13个元素时，哈希表扩容至32。 负载因子越大，空间利用率越高，哈希冲突可能性也越大，所以0.75是这两者考量之下折中的值，并且在这个情况下，哈希冲突同一位置超过8个的概率已经低于千万分之一了 hash函数：hash &amp; (newCap - 1)，即掩码多了一位1，那么要么还在原位置，要么移动原哈希表大小的位置（+oldCap） 7、对比JDK1.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 共享的空Mapstatic final Entry&lt;?,?&gt;[] EMPTY_TABLE = &#123;&#125;;// table就是HashMap实际存储数组的地方transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;public V put(K key, V value) &#123; // 当插入第一个元素的时候，需要先初始化数组大小 if (table == EMPTY_TABLE) &#123; // 数组初始化 inflateTable(threshold); &#125; // 如果key为null，将这个entry放到table[0]中 if (key == null) return putForNullKey(value); // 1. 求key的hash值，进行了多次异或，由于边际效应并不高，1.8只对高16位异或一次 int hash = hash(key); // 2. 找到对应的数组下标 int i = indexFor(hash, table.length); // 3. 遍历一下对应下标处的链表，看是否有重复的key已经存在，如果有，直接覆盖，put方法返回旧值就结束了 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; // key -&gt; value V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 4. 不存在重复的key，将此 entry 添加到链表中 addEntry(hash, key, value, i); return null;&#125;void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果当前HashMap大小已经达到了阈值，并且新值要插入的数组位置已经有元素了，那么要扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 扩容，容量 * 2 resize(2 * table.length); // 扩容以后，重新计算 hash 值 hash = (null != key) ? hash(key) : 0; // 重新计算扩容后的新的下标 bucketIndex = indexFor(hash, table.length); &#125; // 创建元素 createEntry(hash, key, value, bucketIndex);&#125;// 头插法，将新值放到链表的表头，然后size++void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K, V&gt; e = table[bucketIndex]; // 新节点.next = e table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125;void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; // 如果之前的HashMap已经扩充到最大了，那么就将临界值threshold设置为最大的int值 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新的数组 Entry[] newTable = new Entry[newCapacity]; // 将原来数组中的值迁移到新的更大的数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; // 阈值计算 threshold = (int) Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 8、总结Q1：江湖规矩，请问HashMap是线程安全的吗？为什么线程不安全？ Q2：JDK1.7和1.8的HashMap有哪些区别？ ​ Q1、Q2一起回答，当然是线程不安全的； ​ JDK1.7的HashMap添加元素发生哈希冲突时用的是头插法，即插入的元素作为头，这会使得扩容的时候链表逆序，此时多线程可能会产生循环链表，查询一个hash值对应这个循环链表而key又不存在的元素的时候就会陷入死循环中，JDK1.8使用尾插法（preserve order）避免了这种情况。 ​ 当然，JDK1.8的HashMap也是线程不安全的，最简单的一个栗子，if (++size &gt; threshold)，size是一个int类型的成员变量，没有原子性，很显然是线程不安全的，更糟糕的情况还有，如果两个哈希值相同，key不同的元素同时添加，可能会出现一个将另一个直接替换，就导致了数据的丢失。 ​ JDK1.7和1.8的HashMap的区别从数据存储结构来看，1.8多了红黑树的结构，会在哈希表总数&gt;=64 且 同一哈希槽节点数&gt;=8的时候从链表转换为红黑树，红黑树节点&lt;=6时转换成链表，中间隔了个7可以起到一定的缓冲作用。 ​ 1.8还做了一些优化，如hash()只进行了一次异或操作 Q3：如何解决Hash冲突解决？ ​ 上面已经说了，一种是链表，一种是红黑树 Q4：动态扩容的策略是什么？ ​ 指定大小初始化时，哈希表的大小为&gt;=指定大小的2的幂次方 ​ 当size超过扩容阈值（哈希表大小 * 负载因子）时，扩容至原来的两倍，将旧的哈希表数据重新散列到新的哈希表 ​ JDK1.8还有数据量小于64但某个哈希槽数据量&gt;=8时，也会进行扩容","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之LinkedList","slug":"Java源码之LinkedList","date":"2020-02-26T02:49:24.803Z","updated":"2020-02-26T07:40:41.854Z","comments":true,"path":"2020/02/26/Java源码之LinkedList/","link":"","permalink":"https://github.com/linlinnn/2020/02/26/Java%E6%BA%90%E7%A0%81%E4%B9%8BLinkedList/","excerpt":"","text":"Java源码之LinkedList1、带着问题看源码Q1：添加、删除、查找、遍历操作相应操作的原理是什么，时间复杂度是多少？ Q2：跟ArrayList的对比？ Q3：序列化机制？ 2、数据存储结构LinkedList的数据存储结构是一个双向链表 123456789101112131415transient int size = 0;transient Node&lt;E&gt; first; // 指向第一个节点transient Node&lt;E&gt; last; // 指向最后一个节点// 节点private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; // 指向下一个节点 Node&lt;E&gt; prev; // 指向上一个节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; size表示当前链表的节点数，显然可以根据size将链表分成两半，靠近首部就从first开始遍历，靠近尾部就从last开始遍历 3、初始化有两种初始化方法，无参初始化，指定数据集合初始化 1234567public LinkedList() &#123;&#125;public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c);&#125; 4、添加元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public boolean add(E e) &#123; // 在尾部添加一个元素 linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; // 最后一个节点为空，即first节点=last节点 if (l == null) first = newNode; // 添加到最后一个节点的后面 else l.next = newNode; size++; modCount++;&#125;// index表示第一个元素插入的位置public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; // 边界检查是否在[0,size] checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; // 添加元素集合长度为0，直接返回false if (numNew == 0) return false; Node&lt;E&gt; pred, succ; // 前驱节点、后继节点 // 在链尾添加，前驱节点为last节点 if (index == size) &#123; succ = null; pred = last; &#125; else &#123; // 否则，后继节点为index位置所在的节点 succ = node(index); pred = succ.prev; &#125; // 遍历集合插入元素到前驱节点的后面 for (Object o : a) &#123; @SuppressWarnings(\"unchecked\") E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); // 前驱节点为null,插入到链表首部 if (pred == null) first = newNode; // 否则，插入到前驱节点的后面 else pred.next = newNode; // 更新前驱节点 pred = newNode; &#125; // 连接后继节点 // 后继节点为空，即前驱节点为最后一个节点 if (succ == null) &#123; last = pred; &#125; else &#123; // 否则，连接在一起 pred.next = succ; succ.prev = pred; &#125; // 同步链表长度 size += numNew; modCount++; // 修改计数只+1 return true;&#125; 链表操作，边界情况需要仔细考虑，主要考虑头和尾，比如addAll 的逻辑如下 找到index位置对应的前驱节点和后继节点 1.1. 插入到链表尾部，后继节点为null 1.2. 插入到链表头部，前驱节点为null 1.3. 插入到链表中间，后继节点为index节点 将元素集合依次连接在前驱节点的后面 需要考虑1.2这种情况，前驱节点为null，即最后一个新增元素为first节点，然后依次连接 最后一个新增元素与后继节点相连 需要考虑1.1这种情况，后继节点为null，即最后一个新增元素为last节点 5、删除元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 删除头节点private E unlinkFirst(Node&lt;E&gt; f) &#123; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC // 更新头节点为下一个节点 first = next; // 链表只剩一个节点，删除后，first = last = null if (next == null) last = null; // 头节点的prev设为null else next.prev = null; size--; modCount++; return element;&#125;// 删除最后一个节点private E unlinkLast(Node&lt;E&gt; l) &#123; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC // 更新头节点为上一个节点 last = prev; // 链表只剩一个节点，删除后，first = last = null if (prev == null) first = null; // 尾节点的next设为null else prev.next = null; size--; modCount++; return element;&#125;// 删除一个非空节点E unlink(Node&lt;E&gt; x) &#123; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // 和前驱节点断开，考虑删除是不是头节点 if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; // 和后继节点断开，考虑删除是不是尾节点 if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; // 此时该节点的prev、item、next都设为了null size--; modCount++; return element;&#125; 6、查询元素1234567891011121314Node&lt;E&gt; node(int index) &#123; // index位于链表的左半部分，从first开始遍历 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 7、序列化机制12345678910111213141516171819private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException &#123; s.defaultWriteObject(); s.writeInt(size); // 按顺序序列化数据 for (Node&lt;E&gt; x = first; x != null; x = x.next) s.writeObject(x.item);&#125;@SuppressWarnings(\"unchecked\")private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; s.defaultReadObject(); int size = s.readInt(); // 将数据进行反序列化构建链表 for (int i = 0; i &lt; size; i++) linkLast((E)s.readObject());&#125; 8、总结Q1：添加、删除、查找、遍历操作相应操作的原理是什么，时间复杂度是多少？ ​ 添加/删除元素，时间复杂度与位置相关，越靠近头节点或者尾节点，则趋向或等于O(1)，最差就是在中间位置，需要遍历n/2的长度，查找同理；所以遍历需要注意的是要使用迭代器依次顺序遍历，否则如果使用下标随机访问，则每次都要从头节点或者尾结点遍历一遍，大大降低性能。 Q2：跟ArrayList的对比？ 一个是数组结构，一个是双向链表； ArrayList添加数据空间不够时需要动态扩容，LinkedList只需要将数据串联起来 都可以进行随机访问，但是ArrayList才适合，性能比LinkedList高得多 不能笼统地认为插入删除就链表快，查询遍历就数组快，还要考虑具体的位置 把数据插入到头部，LinkedList很快就找到位置并串联起来，但是ArrayList要将元素后移，所以LinkedList快，数据位置越靠近中间LinkedList效率就越差。 把数据插入到尾部，ArrayList需要分情况讨论，不需要扩容的情况下是效率是很高的，因为不用复制移动元素，相比而言LinkedList定位尾部很快，不过需要new对象和指针串连，效率慢了一点，删除同理 查询的话主要是LinkedList不适合随机访问，顺序访问的前提下两者相差不大，ArrayList稍稍占优 Q3：序列化机制？ ​ 双向链表方便于往前和往后遍历，不过也需要花费前驱节点、后继节点这些指针定位空间，而我们真正需要的只是具有顺序性的数据，所以序列化时免去了指针这些不必要的浪费，然后反序列化重新构建链表","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之ArrayList","slug":"Java源码之ArrayList","date":"2020-02-25T01:14:10.654Z","updated":"2020-03-03T15:40:42.823Z","comments":true,"path":"2020/02/25/Java源码之ArrayList/","link":"","permalink":"https://github.com/linlinnn/2020/02/25/Java%E6%BA%90%E7%A0%81%E4%B9%8BArrayList/","excerpt":"","text":"Java源码之ArrayList1、带着问题看源码Q1：添加、查找、遍历操作最为普遍，相应操作的原理是什么，时间复杂度是多少？ Q2：如何进行动态扩展的？ Q3：序列化机制是怎样的？ 2、数据存储结构ArrayList在日常工作中非常常用，底层结构就是一个数组 12345678private static final int DEFAULT_CAPACITY = 10;private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;transient Object[] elementData; // non-private to simplify nested class accessprivate int size; 1、DEFAULT_CAPACITY，初始大小为10 2、EMPTY_ELEMENTDATA 和DEFAULTCAPACITY_EMPTY_ELEMENTDATA 的区别在于添加第一个元素时的扩容策略 3、elementData使用transient修饰，比如一个100万大小的数组，只存储了100个数据，那么只序列化这100个就好了，避免浪费不必要的资源，使用writeObject和readObject进行序列化 3、初始化有三种初始化方法：指定大小初始化，无参初始化，指定数据集合初始化 12345678910111213141516171819202122232425262728293031public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; // 创建一个指定大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; // 返回静态的空数组EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; // 初始大小为负数，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125;public ArrayList() &#123; // 返回静态的空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA，在添加第一个元素后大小才为10 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) // 如果集合元素类型不是Object，转换成Object类型，Q4：为什么？ if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 返回静态的空数组EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; Q4的测试代码如下，转换是为了防止数组元素赋值时发生类型错误 123456List&lt;String&gt; list = Arrays.asList(\"hello,ArrayList\");Object[] arr = list.toArray();System.out.println(arr.getClass().getSimpleName()); // String[]arr[0] = new Object();// java.lang.ArrayStoreException 4、添加元素1234567891011121314151617181920212223242526272829303132333435363738public boolean add(E e) &#123; // 确保数组容量大小，不够时执行扩容 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 如果是静态空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA，在minCapacity和默认大小10取最大值 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 当前数组大小小于期望大小，数组需要扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; // 扩容加上原来的一半大小，即原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 加上一半还是不够大的话就直接扩至期望大小 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 准备扩容大小超过的INT_MAX - 8 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // 为负时说明超出了INT的范围，抛出OutOfMemoryError异常 // 否则MAX_ARRAY_SIZE刚好则为MAX_ARRAY_SIZE，还不够就INT_MAX newCapacity = hugeCapacity(minCapacity); // 将原数组拷贝到新的数组 elementData = Arrays.copyOf(elementData, newCapacity);&#125; 5、删除元素12345678910111213141516171819202122232425public E remove(int index) &#123; // 数组边界检查，只对上限做了检查 rangeCheck(index); // 修改计数+1 modCount++; E oldValue = elementData(index); // 把删除的元素以后的元素向前移动 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125;// Checks if the given index is in range. If not, throws an appropriate runtime exception.private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; ​ 这里值得注意的是rangeCheck和rangeCheckForAdd 的区别，函数的意义决定了函数的职责边界，从而对应内部的实现，如rangeCheck 是get、remove、set方法操作已存在元素的，所以只检查上边界，下边界检查的职责交给数组的访问，而rangeCheckForAdd 是add、addAll操作未存在元素的，所以检查上下边界。 6、迭代器的remove和ArrayList的remove123456789101112131415161718192021222324252627282930public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; // 同步期望计数 expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125;&#125;// 修改计数和期望计数不相同，抛出异常final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125;private void fastRemove(int index) &#123; // 修改计数+1，后续没有进行同步期望计数，在遍历过程中会抛出ConcurrentModificationException modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 遍历删除元素时两种方法比较 通过迭代器Iterator#remove方法不会报错，而forEach调用至fastRemove由于没有同步期望计数，会抛出ConcurrentModificationException 所以这两个remove方法有一定的偏向性，即ArrayList的remove应该用于删除单个元素的场景 7、序列化机制12345678910111213141516171819202122232425262728293031323334353637383940414243private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; int expectedModCount = modCount; // 序列化non-static和non-transient的数据 s.defaultWriteObject(); // Q5:为什么这里还要write一次size呢？ // 这是为了版本兼容，老版本根据size这个成员变量去申请对应空间 // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // 序列化数组元素 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 8、总结Q1：添加、查找、遍历操作最为普遍，相应操作的原理是什么，时间复杂度是多少？ ​ 添加/删除操作，需要考虑具体的位置，在数组开头，需要移动元素，时间复杂度是O(n)，在数组尾部，时间复杂度是O(1)，遍历查找无序数组中的一个元素时间复杂度为O(n) Q2：如何进行动态扩展的？ ​ 当数组空间不足时进行动态扩展，扩展到原数组的1.5倍大小，仍不够的话扩展到期望大小（这种情况是初始化大小为10时或者addAll时发生），最多扩展至INT_MAX Q3：序列化机制是怎样的？ ​ 保存元素的element数组使用transient修饰，是避免序列化没有存储数据的空间提升性能，使用定制化的writeObject序列化和readObject反序列化","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Dubbo扩展点加载机制","slug":"dubbo扩展点","date":"2020-01-11T16:23:25.855Z","updated":"2020-04-11T15:13:40.155Z","comments":true,"path":"2020/01/12/dubbo扩展点/","link":"","permalink":"https://github.com/linlinnn/2020/01/12/dubbo%E6%89%A9%E5%B1%95%E7%82%B9/","excerpt":"","text":"Dubbo扩展点加载机制1、Java SPI使用了策略模式，一个接口多种实现。只声明接口，具体的实现由程序之外的配置控制，用于具体实现的装配。 具体步骤如下： （1）定义一个接口以及对应的方法 （2）编写接口的实现类 （3）在META-INF/services/ 目录下，创建一个接口全限定名命名的文件 （4）文件内容为具体实现类的全限定名，如果有多个，则用分行符分隔 （5）在代码中通过java.util.ServiceLoader 来加载具体的实现类 2、扩展点加载机制的改进 初始化 JDK SPI: 一次性实例化扩展点所有实现，初始化耗时，没有也加载浪费资源 Dubbo SPI: 加载配置文件中的类，并分为不同的种类缓存在内存中，不会立即全部初始化 扩展点加载失败 JDK SPI: 获取不到扩展的名称，不能打印正常的异常信息 Dubbo SPI: 抛出真实异常并打印日志，部分扩展点加载失败不会影响其他扩展点和整个框架的使用 实现了IOC和AOP机制 3、扩展点的配置规范 规范名 规范说明 SPI配置文件路径 META-INF/services/META-INF/dubbo/META-INF/dubbo/internal/全路径类名 文件内容格式 key=value方式，多个用换行符分隔 4、扩展点的分类与缓存Dubbo SPI Class缓存：Dubbo SPI获取扩展类时，先从缓存中读取。如果缓存中不存在，则加载配置文件，根据配置把Class缓存到内存中，不会直接初始化 实例缓存：基于性能考虑，Dubbo框架不仅会缓存Class，也会缓存Class实例化对象。先从缓存中读取，如果缓存中不存在，则重新加载并缓存起来，按需实例化并缓存 扩展类种类 普通扩展类 包装扩展类：Wrapper类没有具体的实现，只是做了通用逻辑的抽象，在构造方法中传入一个具体的扩展接口的实现 自适应扩展类：一个扩展接口有多种实现类，具体实现哪个实现类可以不写死在配置或代码中，在运行时，通过传入URL中的某些参数动态来确定。自适应特性@Adaptive 其他缓存 自适应和自动激活的区别？ isAssignableFrom 和 instanceof 的区别？ 123父类.class.isAssignableFrom(子类.class)子类实例 instanceof 父类类型","categories":[],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://github.com/linlinnn/tags/Dubbo/"}]}]}