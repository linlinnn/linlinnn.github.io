{"meta":{"title":"linjunhua's Blog","subtitle":"","description":"","author":"linlinnn","url":"https://github.com/linlinnn","root":"/"},"pages":[{"title":"About","date":"2020-01-18T12:56:07.352Z","updated":"2020-01-18T12:56:07.352Z","comments":true,"path":"about/index.html","permalink":"https://github.com/linlinnn/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-01-11T14:37:31.000Z","updated":"2020-01-11T14:37:31.921Z","comments":true,"path":"categories/index.html","permalink":"https://github.com/linlinnn/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-01-19T16:29:03.090Z","updated":"2020-01-19T16:29:03.090Z","comments":true,"path":"tags/index.html","permalink":"https://github.com/linlinnn/tags/index.html","excerpt":"","text":""},{"title":"Project","date":"2020-01-19T13:29:02.209Z","updated":"2020-01-18T12:56:07.380Z","comments":true,"path":"project/index.html","permalink":"https://github.com/linlinnn/project/index.html","excerpt":"","text":""}],"posts":[{"title":"深入JVM之java线程基础","slug":"深入JVM之java线程基础","date":"2020-03-08T13:30:12.782Z","updated":"2020-03-08T13:33:36.277Z","comments":true,"path":"2020/03/08/深入JVM之java线程基础/","link":"","permalink":"https://github.com/linlinnn/2020/03/08/%E6%B7%B1%E5%85%A5JVM%E4%B9%8Bjava%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Java线程Java线程状态 新建（New）：创建后未启动的线程 运行（Runnable）：正在执行或者正在等待操作系统为它分配执行时间 无限期等待（Waiting）：等待被其他线程显示唤醒 Object::wait() Thread::join() LockSupport::park() 限期等待（Timed Waiting）：一定时间后由系统自动唤醒 Thread::sleep() Object::wait(Timeout) Thread::join(TImeout) LockSupport::parkNanos() LockSupport::parkUntil() 阻塞（Blocked）：等待获取排它锁 结束（Terminated）：线程已经结束执行 内核线程的调度成本主要来自于用户态与核心态的状态转换，而这两种状态转换的开销主要来自于响应中断、保护和恢复执行现场的成本 线程安全互斥同步​ synchronized持有锁是一个重量级的操作，Java的线程是映射到操作系统的原生内核线程之上的，如果要阻塞或唤醒一条线程，则需要操作系统帮忙完成，这就不可避免地陷入用户态到核心的转换，进行这种状态转换需要耗费很多的处理器时间 ​ 除了synchronized关键字外，自JDK5，基于Lock接口，用户能够以非块结构来实现互斥同步 ​ 重入锁ReentrantLock 是Lock接口的一种实现，它与synchronized很相似，相比增加了一些高级功能，主要由三项：等待可中断、可实现公平锁及锁可以绑定多个条件： 等待可中断：当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。可中断特性对处理执行时间非常长的同步块很有帮助 公平锁：多个线程等待同一个锁时，必须按照申请锁的时间顺序来依次获取锁，而非公平锁不保证这一点，使用非公平锁，将会导致ReentrantLock 性能急剧下降，影响吞吐量 锁绑定多个条件：可以同时绑定多个Condition 对象 synchronized和ReentrantLock 都能够满足需求时，推荐使用synchronized 同步地语义更加清晰 Lock需要对应finally-unlock，但是synchronized会保证释放锁 Lock和synchronized目前性能差不多，不过synchronized将来有更多的优化空间 非阻塞同步​ 互斥同步是一种悲观的并发策略（实际上虚拟机会优化掉很大一部分不必要的加锁），另一个选择是基于冲突检测的乐观并发策略，乐观并发策略需要“硬件指令集的发展”，因为要求操作和冲突检测这两个步骤具备原子性，这类指令常用的有： 测试并设置（Test-and-Set） 获取并增加（Fetch-and-Increment） 交换（Swap） 比较并交换（Compare-and-Swap） 加载链接/条件储存（Load-Linked/Store-Conditonal） CAS指令需要三个操作数，分别是内存位置、旧的预期值、准备设置的新值 12345678910// Unsafe.class// unsafe.getAndAddInt(this, valueOffset, 1) + 1public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 无限循环中，不断尝试将一个比当前值大一的新值赋值给自己，如果失败了，则说明旧值已经发生变化 ABA问题，可以通过一个带有标记原子引用类AtomicStampedReference， 大部分情况下ABA问题不会影响程序并发的正确性，如果要解决ABA问题，改为互斥同步可能会比原子类更为高效 无同步方案线程本地存储（Thread Local Storage），每个线程持有一份变量副本，消除竞争关系 锁优化自旋锁与自适应自旋​ 现在的物理机器普通都是多核处理器系统，能够让两个或以上的能够同时并行执行，可以让后面请求锁的那个线程“稍等一会”，看看持有锁的线程是否很快就会释放锁。为了让线程等待，只需让线程执行一个忙循环（自旋），这项技术就是自旋锁 ​ 自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，所以如果锁被占用的时间很短，自旋等待的效果就好，反之则只会白白消耗处理器资源。因此自旋等待必须有一定的时间限制，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式挂起线程。默认值是十次，-XX:PreBlockSpin设置 ​ 不过无论是默认值还是用户指定的自旋次数，对整个Java虚拟机中所有的锁来说都是相同的。在JDK6中对自旋锁的优化，引入了自适应的自旋，由前一次在同一个锁的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在进行中，那么虚拟机就会认为这个自旋也很有可能再次成功，进而允许自旋等待持续更长的时间，比如持续100次忙循环。另一方面，如果对于某个锁，自旋很少成功获得过锁，那么以后就有可能忽略掉自旋的过程，直接挂起 锁消除​ 锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除，主要依据是逃逸分析的数据支持。 锁粗化​ 虚拟机检测到一串操作都对同一对象的加锁，放大加锁的范围到整个操作序列的外部 轻量级锁​ 在没有多线程竞争的前提下（大部分的锁，在整个同步周期内都不存在竞争），减少传统的重量级锁使用操作系统互斥量产生的性能消耗 偏向锁​ 消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能，即对比轻量级锁连CAS操作都省去","categories":[],"tags":[{"name":"深入JVM","slug":"深入JVM","permalink":"https://github.com/linlinnn/tags/%E6%B7%B1%E5%85%A5JVM/"}]},{"title":"网络通信之TCP","slug":"网络通信之TCP","date":"2020-03-08T13:20:52.339Z","updated":"2020-03-08T13:29:06.185Z","comments":true,"path":"2020/03/08/网络通信之TCP/","link":"","permalink":"https://github.com/linlinnn/2020/03/08/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E4%B9%8BTCP/","excerpt":"","text":"网络通信之TCP1.1、简介： 面向连接的、可靠的、基于字节流的传输层通信协议 将应用层的数据流分割成报文段并发送给目标节点的TCP层，数据包都有序号，对方收到则发送ACK确认，保证有序接收、重复报文自动废弃、未收到则重传 使用奇偶校验和来检验数据在传输过程中是否有误 双向传递（全双工） 流量缓冲：解决速度不匹配问题 拥塞控制 1.2、TCP报文 TCP Flags ACK: 确认序号标志 SYN: 请求连接标志 FIN: 释放连接标志 PSH:数据传输标志 1.3、三次握手 1.3.1、SYN报文 第一次握手：SYN=1，随机生成seq序号989512128（） Client 状态：CLOSED =&gt; SYN_SENT Server状态：LISTEN =&gt; SYN_REVD 1.3.2、SYN/ACK报文 第二次握手：SYN=1，ACK=1, ack=seq+1=989512129, 随机生成seq序号753743756 Client 状态：SYN_SENT =&gt; ESTALISHED 1.3.3、ACK报文 第三次握手：ACK=1，ack=seq+1=753743757 Server状态：SYN_REVD =&gt; ESTABLISHED，成功建立连接，可以开始数据传输 为什么两次握手不可以？ 假设是两次握手，客户端发送了SYN报文1，但没有收到SYN/ACK报文，以为报文丢失，重新发送SYN报文2，此时连接建立然后关闭，但SYN报文1 此时到达了服务端，再次建立连接，导致错误连接和资源浪费。 而三次握手即使服务端收到了失效的SYN报文1 ，由于客户端不会发送ACK报文，此时服务端收不到ACK报文，就不会建立连接 1.3.4、PSH/ACK报文 ACK=1，PSH=1，开始传输数据，数据长度为285 1.3.5、ACK报文 ACK=1，ack=seq+len=989512129+285=989512414 1.4、SYN攻击​ 攻击者短时间伪造不同IP地址的SYN报文，快速占满backlog队列，服务器端的连接都处于SYN_RECEIVED的状态，占用大量的资源，使得正常的连接无法建立。 net.core.netdev_max_backlog 接受自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn SYN_RCVD状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的SYN直接回包RST，丢弃连接 net.ipv4.tcp_syncookies = 1 当SYN队列满后，新的SYN不进入队列，计算出cookie再以SYN+ACK的序列号返回客户端，正常客户端发报文时，服务器根据报文中携带的cookie重新恢复连接 由于cookie占用序列号空间，导致此时所有TCP可选功能失效 1.4、三次握手优化1.4.1、操作系统内核限制调整 在操作系统内核中分别用SYN队列和ACCEPT队列来维护相应的连接套接字 当连接数非常多的时候可以对操作系统内核限制进行调整 服务器端SYN_RCV状态 net.ipv4.tcp_max_syn_backlog：SYN_RCVD状态连接的最大个数 net.ipv4.tcp_synack_retries：被动建立连接时，发SYN/ACK的重试次数 客户端SYN_SENT状态 net.ipv4.tcp_syn_retries = 6 主动建立连接时，发SYN的重试次数 net.ipv4.ip_local_port_range = 32768 ~ 60999 建立连接时的本地端口可用范围 ACCEPT队列设置 TCP_DEFER_ACCEPT：当有数据报文时操作系统内核才激活应用程序 1.4.2、Fast Open 降低时延 每一次TCP连接数据传输都需要2*RTT时间 TCP协议提供了Fast Open方式，服务器将第一次建立连接成功的相关信息保存在Cookie缓存在客户端中，客户端后续请求建立连接通过携带这个Cookie消除了三次握手，使得只需要1*RTT的时间，若Cookie丢失，则在RTO后发起普通的三次握手连接 Server校验Cookie（解密Cookie以及比对IP地址或者重新加密IP地址以和接收到的Cookie进行对比）。 如果验证成功，向用户发送SYN+ACK，在用户回复ACK之前，便可以向用户传输数据； 如果验证失败，则丢弃此TFO请求携带的数据，回复SYN-ACK确认SYN Seq，完成正常的三次握手。 建立了TFO连接而又没有完成TCP连接的请求在Server端被称为pending TFO connection，当pending的连接超过上限值，Server会关闭TFO，后续的请求会按正常的三次握手处理。 如果一个带有TFO的SYN请求如果在一段时间内没有收到回应，用户会重新发送一个标准的SYN请求，不带 任何其他数据。 Linux上打开TCP Fast Open net.ipv4.tcp_fastopen：系统开启TFP功能 0： 关闭 1：作为客户端时可以使用TFO 2：作为服务器时可以使用TFO 3：无论作为客户端还是服务器，都可以使用TFO 1.5、四次挥手 为什么建立连接时三次握手，释放连接却需要四次挥手？ 客户端发送FIN报文表示客户端不再发送数据（FIN-WAIT-1状态），但可以接收数据，服务端也还可以发送数据 服务端发送ACK报文，进入FIN-WAIT-2状态，服务端进入CLOSE-WAIT状态 服务端将最后的数据发送完毕，发送FIN报文，进入LAST-ACK状态 客户端发送ACK报文， 进入TIME-WAIT状态，此时TCP连接还没有释放，必须经过2*MSL时间后，当客户端撤销相应的TCB后，才进入CLOSED状态 服务端只要受到客户端发出的确认，立即进入CLOSED状态，撤销TCB，结束TCP连接 为什么客户端发送ACK报文后要等待2*MSL（Maximum Segment Lifetime）时间？ 保证客户端发送的ACK报文可以到达服务端，假设此报文丢失，服务端就会认为客户端没有接收到FIN报文，进行重发，客户端就可以在2MSL时间内收到重发的报文，发送ACK报文，重新等待2MSL时间 在2*MSL时间内产生的报文都会在网络中消失，防止新连接接收到就连接的请求报文 大量的连接处于TIME_WAIT状态，导致部分客户端连接不上？ 主要关注场景：服务端主动关闭连接，高并发短连接 /etc/sysctl.conf -p让参数生效 /etc/sysctl.conf是一个允许改变正在运行中的Linux系统的接口，它包含一些TCP/IP堆栈和虚拟内存系统的高级选项，修改内核参数永久生效。 net.ipv4.tcp_tw_reuse = 1 表示开启重用。（客户端）允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 由于timestamp的存在，操作系统可以拒绝迟到的报文 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_tw_recycle = 1 （客户端和服务端）表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 不安全，无法避免报文延迟、重复等给新连接造成混乱 net.ipv4.tcp_fin_timeout 修改默认的 TIMEOUT 时间 net.ipv4.tcp_keepalive_time = 1200 表示当keepalive起用的时候，TCP发送keepalive消息的频度，缺省是2小时，改为20分钟 net.ipv4.ip_local_port_range = 1024 65000 表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000 net.ipv4.tcp_max_syn_backlog = 8192 表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数 net.ipv4.tcp_max_tw_buckets = 5000 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息 默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于 Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死 大量连接处于CLOSE_WAIT状态 主要关注场景：一般是程序没有正常地释放资源导致 分析思路： 出现问题后，立马应该检查日志，确实日志没有发现问题； 监控明确显示了socket不断增长，很明确立马应该使用 netstat 检查情况看看是哪个进程的锅； 根据 netstat 的检查，使用 tcpdump 抓包分析一下为什么连接会被动断开； 如果熟悉代码应该直接去检查业务代码，如果不熟悉则可以使用 perf 把代码的调用链路打印出来； 不论是分析代码还是火焰图，到此应该能够很快定位到问题。 SO_REUSEADDR可以用在以下四种情况下。 (摘自《Unix网络编程》卷一) 1、当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你启动的程序的socket2要占用该地址和端口，你的程序就要用到该选项。 2、SO_REUSEADDR允许同一port上启动同一服务器的多个实例(多个进程)。但每个实例绑定的IP地址是不能相同的。在有多块网卡或用IP Alias技术的机器可以测试这种情况。 3、SO_REUSEADDR允许单个进程绑定相同的端口到多个socket上，但每个socket绑定的ip地址不同。这和2很相似，区别请看UNPv1。 4、SO_REUSEADDR允许完全相同的地址和端口的重复绑定。但这只用于UDP的多播，不用于TCP。 1.6、滑动窗口（流量控制）1、通过一个最新ack序列号来确认在这之间的报文都以确认接收，丢包只需重发可发送窗口内ack序列号之后的报文。 2、接收方根据自己的处理情况返回下次发送多少个报文来控制滑动窗口大小，为避免该报文丢失，发送方会定时发送一个窗口探测报文。 1.7、netstat命令查看TCP状态1234567netstat -a 显示所有连接和监听端口-n 以数字形式（如IP地址）显示地址和端口号-r 显示路由表-s 显示每个协议的统计信息-o(Windows) 显示拥有的与每个连接关联的进程ID-b(Windows)/-p(Linux) 显示对应的可执行程序的名字","categories":[],"tags":[{"name":"网络通信","slug":"网络通信","permalink":"https://github.com/linlinnn/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"}]},{"title":"代码精进之路——读书笔记（1）","slug":"代码精进之路——读书笔记（1）","date":"2020-03-08T04:00:07.520Z","updated":"2020-03-08T11:08:48.703Z","comments":true,"path":"2020/03/08/代码精进之路——读书笔记（1）/","link":"","permalink":"https://github.com/linlinnn/2020/03/08/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/","excerpt":"","text":"代码精进之路——读书笔记（1）1、前言​ 很多时候看到自己写代码或是别人写的代码就觉得要吐了，可是又很困惑该如何改进，想起一首rap，当我总是被这些问题给缠住，慢慢地想要克服它变成一种难度，周末的时候买了本新书《代码精进之路》，原来这是很多程序员都会遇到的情况，软件的复杂性是一个基本特征，再加上源源不断的需求压力总是使开发人员妥协，引发破窗效应，系统越来越混乱。故此记录下一些很有启发的理念。 2、命名规范2.1、代码注释​ 如果注释是为了阐述代码背后的意图，那么这个注释是有意义的； ​ 如果注释是为了复述代码功能，那么有可能就意味着代码的坏味道； ​ 好的代码通过好的命名规范使得自身意图是显性化的，就如同看一篇说明文。 123456789101112// 复述代码功能：线程休眠2秒// 阐述代码背后的意图：线程休眠2秒，为了等待相关系统处理结果Thread.sleep(2000);// 代码自释：不需要注释也能清楚获悉代码本意private void waitProcessResultFromA()&#123; try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; LOGGER.error(e); &#125;&#125; 3、函数3.1、单一职责法则​ SRP（Single Responsibility Principle）单一职责法则，现阶段自身的系统抽象能力不足，还需要很多的经验和总结，但是将这一法则作用于函数是每个程序员都能够做到的，而将函数写好了，系统也会开始往好的方向衍进。 ​ 一个函数由命名、入参、出参组成，只要明确这三个点的意义，函数实现起来也会变得清晰，拿leetcode上一道简单题打个比方。 ​ 一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 ​ 以前遇到这种问题总是会陷入一种难以言喻的递归模拟，但其实函数的职责是很明确的，返回n级台阶的跳法 总数，入参为台阶级数，递归边界就是1级台阶只有1种跳法，2级台阶有两种跳法。 123456// 函数意义：台阶级数为floor层的跳法private void jumpWays(int floor)&#123; if(floor &lt;= 2) return floor; // 翻译一下递归就是，跳一级，也可以跳两级，返回总共的跳法 return jumpWays(floor - 1) + jumpWays(floor - 2);&#125; ​ 当然还要对递归子问题使用记忆化进行优化。 ​ 这样就有一种感觉，写代码就是将需求以合适的方式复述出来，是一篇说明文。 3.2、优化判空​ NPE(Null Pointer Exception)是需要时刻注意判断的，这就让要获取一些属性链路比较长的属性时经常写出这种令人呕吐的代码，可以使用Java 8的新特性Optional 进行优化 12345678910111213141516// 为了获取user里的isocodeif(user != null)&#123; Address address = user.getAddress(); if(address != null)&#123; Country country = address.getCountry(); if(country != null)&#123; String isocode = country.getIsocode(); &#125; &#125;&#125;// 比起大量的if语句，这样的链式调用更符合思维String isocode = Optional.ofNullable(user) .flatMap(User::getAddress) .flatMap(Address::getCountry) .map(Country::getIsocode) .orElse(\"default\"); 3.3、组合函数模式​ 组合函数有助于代码保持精炼并易于复用，这样的代码就像一本很多说明文组成的书，入口函数是目录，目录的内容指向具体的私有函数。 ​ Spring中BeanUtils#copyProperties 方法有57行，太长的函数容易让人迷失陷入细节，增大理解难度。当然细节是很重要的，只不过一次只关注一个点更利于提高效率，把握住重点，经过分析，这个函数主要做了两件事：一、判断能不能copy，二、执行copy。因此入口函数拆分为两个步骤。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public void copyProperties(Object dest, Object orig) throws IllegalAccessException, InvocationTargetException &#123; // 1、checkDestAndOrig(dest, orig); 判断能不能copy // ========================================================= // Validate existence of the specified beans if (dest == null) &#123; throw new IllegalArgumentException (\"No destination bean specified\"); &#125; if (orig == null) &#123; throw new IllegalArgumentException(\"No origin bean specified\"); &#125; if (log.isDebugEnabled()) &#123; log.debug(\"BeanUtils.copyProperties(\" + dest + \", \" + orig + \")\"); &#125; // 2、copyOrigToDest(orig, dest); 执行copy，这段逻辑还是稍微长了，可以继续划分 // ========================================================= // Copy the properties, converting as necessary if (orig instanceof DynaBean) &#123; DynaProperty origDescriptors[] = ((DynaBean) orig).getDynaClass().getDynaProperties(); for (int i = 0; i &lt; origDescriptors.length; i++) &#123; String name = origDescriptors[i].getName(); if (getPropertyUtils().isWriteable(dest, name)) &#123; Object value = ((DynaBean) orig).get(name); copyProperty(dest, name, value); &#125; &#125; &#125; else if (orig instanceof Map) &#123; Iterator names = ((Map) orig).keySet().iterator(); while (names.hasNext()) &#123; String name = (String) names.next(); if (getPropertyUtils().isWriteable(dest, name)) &#123; Object value = ((Map) orig).get(name); copyProperty(dest, name, value); &#125; &#125; &#125; else /* if (orig is a standard JavaBean) */ &#123; PropertyDescriptor origDescriptors[] = getPropertyUtils().getPropertyDescriptors(orig); for (int i = 0; i &lt; origDescriptors.length; i++) &#123; String name = origDescriptors[i].getName(); if (\"class\".equals(name)) &#123; continue; // No point in trying to set an object's class &#125; if (getPropertyUtils().isReadable(orig, name) &amp;&amp; getPropertyUtils().isWriteable(dest, name)) &#123; try &#123; Object value = getPropertyUtils().getSimpleProperty(orig, name); copyProperty(dest, name, value); &#125; catch (NoSuchMethodException e) &#123; ; // Should not happen &#125; &#125; &#125; &#125;&#125; ​ 很多时候只要多做一点点，就可以写出更好的代码。","categories":[],"tags":[{"name":"代码精进之路","slug":"代码精进之路","permalink":"https://github.com/linlinnn/tags/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF/"}]},{"title":"Java源码之ThreadPoolExecutor","slug":"Java源码之ThreadPoolExecutor","date":"2020-03-02T23:49:50.542Z","updated":"2020-03-08T13:23:04.036Z","comments":true,"path":"2020/03/03/Java源码之ThreadPoolExecutor/","link":"","permalink":"https://github.com/linlinnn/2020/03/03/Java%E6%BA%90%E7%A0%81%E4%B9%8BThreadPoolExecutor/","excerpt":"","text":"Java源码之ThreadPoolExecutor1、带着问题看源码Q1：使用线程池的好处是什么？ Q2：线程池的实现原理，工作流程？ Q3：执行完任务后的线程会怎样？ 2、成员变量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 线程池的控制状态，由两部分构成：1、工作线程数，2、线程池状态private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));// 限制工作线程数为 2^29 - 1, Integer.SIZE - 3 = 32 - 3 = 29private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// 线程池的状态// 接受新的任务和处理队列里的任务 private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 不再接受新的任务，但会继续处理队列中已存在的任务private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// 不再接受新的任务，也不再处理队列中已存在的任务，并且中断运行中的任务private static final int STOP = 1 &lt;&lt; COUNT_BITS;// 所有的任务中断，工作线程数为0，线程过渡到整理状态，运行terminated回调函数private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// 线程池收工private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;// 等待处理的工作阻塞队列private final BlockingQueue&lt;Runnable&gt; workQueue;// 可重入锁，mainLockprivate final ReentrantLock mainLock = new ReentrantLock();// 工作线程集合private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();// 用来支持awaitTermination的状态private final Condition termination = mainLock.newCondition();// 线程池最大容量private int largestPoolSize;// 已处理任务数private long completedTaskCount;// 线程工厂private volatile ThreadFactory threadFactory;// 拒绝策略private volatile RejectedExecutionHandler handler;// 线程最大空闲时间private volatile long keepAliveTime;// 默认为false, 核心线程超过最大空闲时间仍然存活，为true时则会被回收private volatile boolean allowCoreThreadTimeOut;// 核心线程数大小private volatile int corePoolSize;// 线程池最大线程数private volatile int maximumPoolSize;// 默认拒绝策略AbortPolicyprivate static final RejectedExecutionHandler defaultHandler = new AbortPolicy();private static final RuntimePermission shutdownPerm = new RuntimePermission(\"modifyThread\");/* The context to be used when executing the finalizer, or null. */private final AccessControlContext acc; 3、构造方法","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之ConcurrentHashMap","slug":"Java源码之ConcurrentHashMap","date":"2020-02-27T14:49:23.769Z","updated":"2020-03-08T04:24:53.727Z","comments":true,"path":"2020/02/27/Java源码之ConcurrentHashMap/","link":"","permalink":"https://github.com/linlinnn/2020/02/27/Java%E6%BA%90%E7%A0%81%E4%B9%8BConcurrentHashMap/","excerpt":"","text":"Java源码之ConcurrentHashMap1、带着问题看源码Q1：ConcurrentHashMap为什么是线程安全的？ Q2：JDK1.7和1.8的ConcurrentHashMap有什么不同？ Q3：动态扩容策略是什么，如何多线程扩容的？ 2、数据存储结构先看JDK1.8的，数组 + 链表 + 红黑树，为了保证扩容时的线程安全，还增加了一种转移节点 123456789101112131415161718192021222324// 扩容转移时的最小数组分组大小private static final int MIN_TRANSFER_STRIDE = 16;static final int MOVED = -1; // hash for 转移节点static final int TREEBIN = -2; // hash for 红黑树根节点static final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash// CPU核数static final int NCPU = Runtime.getRuntime().availableProcessors();// volatile 修饰哈希表，具有线程可见性transient volatile Node&lt;K,V&gt;[] table;// 扩容哈希表private transient volatile Node&lt;K,V&gt;[] nextTable;/** * Table initialization and resizing control. * -1: 初始化 * -(1 + 正在进行resize的线程数)，与-1区别开 * 0: 默认状态 * 在初始化之后，该值表示下一次扩容阈值 */private transient volatile int sizeCtl;// 转移下标private transient volatile int transferIndex;// 转移节点static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123;&#125; 3、初始化自旋 + cas + 双重check 1234567891011121314151617181920212223242526272829//初始化 table，通过对 sizeCtl 的变量赋值来保证数组只能被初始化一次private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; // 通过自旋保证初始化成功 while ((tab = table) == null || tab.length == 0) &#123; // 小于0代表有线程正在初始化，释放当前CPU的调度权 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 赋值保证当前只有一个线程在初始化，-1 代表当前只有一个线程能初始化 // 保证了数组的初始化的安全性 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; // 可能执行到这里的时候，table已经不为空了，这里是双重check if ((tab = table) == null || tab.length == 0) &#123; // 进行初始化 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); // 负载因子0.75 &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 3、查询元素不需要加锁，因为哈希表的节点用volatile修饰了，多线程之间具有可见性 12345678910111213141516171819202122232425public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 计算hashcode int h = spread(key.hashCode()); // 不是空的数组 &amp;&amp; 并且当前索引的槽点数据不是空的 // 否则该key对应的值不存在，返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 槽点第一个值和key相等，直接返回 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 如果是红黑树或者转移节点，使用对应的find方法 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 如果是链表，遍历查找 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 4、添加元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // HashMap时可以put null的，ConcurrentHashMap不可以 if (key == null || value == null) throw new NullPointerException(); // h ^ (h &gt;&gt;&gt; 16) &amp; HASH_BITS(0x7fffffff)，多&amp;了HASHBITS // hash的负在ConcurrentHashMap中有特殊意义表示在扩容或者是树节点 int hash = spread(key.hashCode()); int binCount = 0; // 当对应的哈希槽为转移节点时陷入自旋，等待扩容完成 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 哈希表为空，初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 如果当前索引位置为空，直接插入 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //cas在位置i创建新的元素，当位置i为空时，即能创建成功，结束自旋 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 如果当前哈希槽是转移节点，表示该槽点正在扩容，就会一直等待扩容完成 // 转移节点的hash值为MOVED=-1 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); // 哈希槽上有值，即发生冲突 else &#123; V oldVal = null; // 锁定当前哈希槽，其余线程不能操作，保证了安全 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; // 链表 if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // key已存在 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 记录旧值 oldVal = e.val; // 覆盖值 if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 把新增的元素添加到链表的末尾 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 红黑树，这里没有使用TreeNode,使用的是TreeBin，TreeNode只是红黑树的一个节点 // TreeBin持有红黑树的引用，并且会对其加锁，保证其操作的线程安全 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; // 在putTreeVal方法里面，在给红黑树重新着色旋转的时候，会锁住红黑树的根节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; // 记录旧值 oldVal = p.val; // 覆盖值 if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // binCount不为0 if (binCount != 0) &#123; // 链表是否需要转化成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); // 旧值不为空，返回旧值 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 哈希槽数 +1 addCount(1L, binCount); return null;&#125; 5、动态扩容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240//新增元素时，也就是在调用 putVal 方法后，为了通用，增加了check入参，用于指定是否可能会出现扩容的情况//check &gt;= 0 即为可能出现扩容的情况，例如 putVal方法中的调用private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // 类似LongAdder的计数 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // 检查当前哈希表元素个数s是否达到扩容阈值sizeCtl ，扩容时sizeCtl为负数，依旧成立，同时还得满足数组非空且数组长度不能大于允许的数组最大长度这两个条件 // 这个while循环除了判断是否达到阈值从而进行扩容操作之外还有一个作用就是当一条线程完成自己的迁移任务后，如果集合还在扩容，则会继续循环，帮助扩容，申请后面的迁移任务 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; // 根据length得到一个标识 int rs = resizeStamp(n); // sc &lt; 0说明集合正在扩容当中 if (sc &lt; 0) &#123; // 如果 sc 的高 16 位不等于标识符（说明sizeCtl变化了） // 如果 sc == 标识符 + 1（当一个扩容线程结束，就会将sc减一，全部扩容线程结束，sc就等于rs+1 ） // 如果 sc == 标识符 + MAX_RESIZERS（65535，即低16全为1，扩容线程数已经达到最大） // 如果 nextTable == null（结束扩容了） // 如果 transferIndex &lt;= 0 (转移状态变化了) // 结束循环 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 扩容还未结束，并且允许扩容线程加入，扩容线程数+1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 如果不在扩容，将sc更新：标识符左移16位 然后+2. 也就是变成一个负数。高16位是标识符，低16位初始是2（即为一个线程），后面扩容时会根据线程是否为这个值来确定是否为最后一个线程 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 更新sizeCtl为负数后，开始扩容 transfer(tab, null); s = sumCount(); &#125; &#125;&#125;// 扩容状态下其他线程对集合进行插入、修改、删除、合并、compute等操作时遇到 ForwardingNode 节点会调用该帮助扩容方法final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); //此处的 while 循环是上面 addCount 方法的部分逻辑 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125;// tab：原数组，nextTab：新数组private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; // 原数组的长度 int n = tab.length, stride; // 将 length / 8 然后除以 CPU核心数。如果得到的结果小于MIN_TRANSFER_STRIDE（16），那么就等于16。 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果新数组为空，初始化 if (nextTab == null) &#123; // initiating try &#123; // 扩容两倍 @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME // 扩容失败， sizeCtl设为INT_MAX sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; // transferIndex 表示转移时的下标边界 transferIndex = n; &#125; // 新数组的长度 int nextn = nextTab.length; // 代表转移节点，如果原数组上是转移节点，说明该节点正在被扩容 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 推进标识 boolean advance = true; // 标识是否完成 boolean finishing = false; // to ensure sweep before committing nextTab // 自旋，i 的值会从原数组划分的子区域的最大值开始，慢慢递减到 0 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; // 对 i 减一，判断是否大于等于 bound （正常情况下，如果大于 bound 不成立，说明该线程上次领取的任务已经完成了。那么，需要在下面继续领取任务） // 如果对 i 减一大于等于 bound（还需要继续做任务），或者完成了，修改推进状态为 false，不能推进了。任务成功后修改推进状态为 true。 // 通常，第一次进入循环，i-- 这个判断会无法通过，从而走下面的 nextIndex 赋值操作（获取最新的转移下标）。其余情况都是：如果可以推进，将 i 减一，然后修改成不可推进。如果 i 对应的桶处理成功了，改成可以推进。 if (--i &gt;= bound || finishing) advance = false; // 这里的目的是：1. 当一个线程进入时，会选取最新的转移下标。2. 当一个线程处理完自己的区间时，如果还有剩余区间的没有别的线程处理。再次获取区间。 else if ((nextIndex = transferIndex) &lt;= 0) &#123; // 如果小于等于0，说明没有区间了 ，i 改成 -1，推进状态变成 false，不再推进，表示，扩容结束了，当前线程可以退出了 i = -1; advance = false; &#125; // CAS 修改 transferIndex，即 length - 区间值，留下剩余的区间值供后面的线程使用 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; // 其中一个条件满足说明拷贝结束了 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; // 置空 table = nextTab; // 更新哈希表 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // 更新阈值 return; &#125; // 尝试将 sc -1. 表示这个线程结束扩容了，将 sc 的低 16 位减一。 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 如果sc - 2不等于标识符左移 16 位。如果相等了，说明没有线程在帮助扩容了,也就是说，扩容结束了。 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return;// 不相等，说明还有其他扩容线程，当前线程结束方法。 finishing = advance = true; // 相等，标记结束 i = n; // recheck before commit &#125; &#125; // 原哈希表位置i为空，用fwd转移节点占位 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 为转移节点，说明别的线程已经处理过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed // 到这里，说明这个位置有实际值了，且不是占位符。对这个节点加锁。加锁是为了防止 putVal 的时候向链表插入数据 else &#123; synchronized (f) &#123; // 判断 i 下标处的桶节点是否和 f 相同 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // low node, high node if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; // 记录上一个节点 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // 如果最后更新的runBit为0，设为low节点，为1则设为high节点 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; // 再次循环，生成两个链表，lastRun作为停止条件，为了避免多余的新建节点连接 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 在新数组相应位置上放置拷贝的值，原理和hashmap一样 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); // 在旧数组位置上放上 ForwardingNode 节点 // put 时，发现是 ForwardingNode 节点，就不会再动这个节点的数据了 setTabAt(tab, i, fwd); advance = true; &#125; // 红黑树的拷贝，同 HashMap 的内容 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; // 在旧数组位置上放上 ForwardingNode 节点 setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 看完之后觉得十分巧妙，引入一个转移节点，resize扩容的并发线程数与ConcurrentHashMap的操作频繁度相关，越频繁，并发扩容线程数越多，非常合理极致地利用CPU资源。 6、对比JDK1.76.1、数据存储结构数组（Segment） + 数组（HashEntry） + 链表（HashEntry节点） 12345// 默认并发度16static final int MAX_SEGMENTS = 1 &lt;&lt; 16;// 分段锁static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123;&#125;transient volatile HashEntry&lt;K,V&gt;[] table; 6.2、查询元素由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。 12345678910111213141516171819public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 计算hash值，对应具体的Segment int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; // 遍历元素，找到了就返回 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 6.3、添加元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); // 根据key找Segment int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false);&#125;final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); // put时锁定。如果当前没这条数据，则会返回新创建的HashEntry，否则为空 V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 返回数组中对应位置的元素（链表头部） // 遍历 for (HashEntry&lt;K,V&gt; e = first;;) &#123; // 节点不为空 if (e != null) &#123; K k; // key是否相等，是否进行覆盖 if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; // 如果数组对应位置为空 if (node != null) // 非空，则表示为新创建的值，头插法 node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); // 否则创建一个 int c = count + 1; if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 超过了容量阈值，但没达到最大限制，则扩容table else setEntryAt(tab, index, node); // 直接用新的node，替换掉旧的first node ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue;&#125;private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); // 返回与hash对应的数组内容 HashEntry&lt;K,V&gt; e = first;// 数组对应位置的链表头部 HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node while (!tryLock()) &#123;// 非阻塞获取锁 HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; // 如果头节点为空，创建新node，作为链表头部 if (node == null) // speculatively create node node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) // 存在，则比较是否为同一个key retries = 0; else // 存在，且链表头部和当前插入的值非同，则比较。 e = e.next; &#125; else if (++retries &gt; MAX_SCAN_RETRIES) &#123; // 最多tryLock次数，超过次数，就阻塞 lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 6.4、动态扩容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) &#123; // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot // 与1.8类似 HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; newTable[lastIdx] = lastRun; for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; // 相当于 i 或 i + n HashEntry&lt;K,V&gt; n = newTable[k]; // 头插法 newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 插入新节点 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 7、总结Q1：ConcurrentHashMap为什么是线程安全的？ Q2：JDK1.7和1.8的ConcurrentHashMap有什么不同？ 1、锁粒度和使用锁 JDK1.7的ConcurrentHashMap通过分段锁机制减小锁的粒度，使用可重入锁ReentrantLock保证线程安全 JDK1.8进一步缩小了锁的粒度至哈希槽，使用CAS机制和Synchronized（锁优化）保证线程安全 2、并行度控制 JDK1.7通过分段锁的个数控制并发，默认是16 JDK1.8通过转移节点，扩容时并行度可以根据容器操作的频率进行动态的调整 3、数据结构 ​ 和HashMap一样，JDK1.8增加了红黑树和链表的转换，主要是兜底哈希函数效果不好的时候，查询性能至少能维持在O(logn) Q3：动态扩容策略是什么，如何多线程扩容的？ ​ 扩容至原来的两倍，对原哈希表的数据进行重新散列 ​ 转移节点和sizeCtl标识十分重要，当一个哈希槽的节点为转移节点时，put操作的线程会进行等待或帮助扩容，每个线程领取哈希表中的一段进行转移，转移完成后领取剩余断，扩容结束后更新sizeCtl，从而根据put的频繁程度并行度也会相应地动态变化。","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之HashMap","slug":"Java源码之HashMap","date":"2020-02-26T07:51:41.426Z","updated":"2020-02-27T16:03:15.159Z","comments":true,"path":"2020/02/26/Java源码之HashMap/","link":"","permalink":"https://github.com/linlinnn/2020/02/26/Java%E6%BA%90%E7%A0%81%E4%B9%8BHashMap/","excerpt":"","text":"Java源码之HashMap1、带着问题看源码Q1：江湖规矩，请问HashMap是线程安全的吗？为什么线程不安全？ Q2：JDK1.7和1.8的HashMap有哪些区别？ Q3：如何解决Hash冲突解决？ Q4：动态扩容的策略是什么？ 2、数据存储结构先从1.8的看起，HashMap底层的数据结构是数组+链表+红黑树 显然最突出就是一个链表和红黑树的转换 12345678910111213141516171819202122// 默认的HashMap的空间大小16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// hashMap最大的空间大小static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// HashMap默认负载因子，负载因子越小，hash冲突机率越低，至于为什么，看完下面源码就知道了static final float DEFAULT_LOAD_FACTOR = 0.75f;// 链表长度大于等于8时，链表转化成红黑树static final int TREEIFY_THRESHOLD = 8;// 红黑树大小小于等于6时，红黑树转化成链表static final int UNTREEIFY_THRESHOLD = 6;// 当数组容量大于 64 时，链表才会转化成红黑树static final int MIN_TREEIFY_CAPACITY = 64;// 存放数据的数组transient Node&lt;K,V&gt;[] table;// 临界值（超过这个值则开始扩容）int threshold;// HashMap 负载因子final float loadFactor;// 链表的节点static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;// 红黑树的节点static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; 3、初始化有四种初始化方法，指定初始容量和负载因子，指定初始容量，无参，指定数据集合 12345678910111213141516171819202122232425262728293031323334353637public HashMap(int initialCapacity, float loadFactor) &#123; // 边界值判断 if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; // 根据初始容量计算扩容临界值，&gt;=initialCapacity的2的最小的幂次方 this.threshold = tableSizeFor(initialCapacity);&#125;// 注意这里没有32位的移动，因为最大容量MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; 4、查询元素1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; // tab:哈希表，first:hash对应哈希槽，n:哈希表的长度， Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 第一个元素是不是想要查询的？是的话直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 遍历查询 if ((e = first.next) != null) &#123; // 红黑树，BST查询 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 链表，遍历查询 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 知道元素时怎样查的了，那么添加元素肯定就要与之对应起来。 5、添加元素12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; // key为null返回0，反则hash码后与自身高16位进行异或 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; // tab:哈希表，n:哈希表的长度，i:散列后落到哈希表的索引，p：索引i对应的节点 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果哈希表为空，进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果节点为空，直接设为新节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 旧节点和新节点哈希值和key都相同 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 节点p是一个红黑树节点，按照红黑树的方式设置 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 是一个链表 for (int binCount = 0; ; ++binCount) &#123; // 遍历链表至末尾 if ((e = p.next) == null) &#123; // 新节点插入到链表尾部 p.next = newNode(hash, key, value, null); // 链表长度超过阈值8的话，转换成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 循环过程中遇到一个与新节点相同的节点，退出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // e不为空代表有一个节点哈希值和key都和新节点相同 if (e != null) &#123; // existing mapping for key V oldValue = e.value; // 如果不是putIfAbsent或者旧值为空，替换为新值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 哈希表超过扩容阈值，进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; ​ 红黑树是一种平衡二叉查找树，查询效率为O(logn) ​ 为什么大于等于8要进行链表到红黑树的转换，注释上大佬有说，因为正常情况下根据泊松分布，有8个哈希值冲突的概率是0.00000006，更多的话就小于千万分之一，一般根本不会发生，之后是小弟的认知，以此推论如果发生了的话就是散列函数出了问题或者是数据非常特殊导致不断发生哈希冲突，在这种情况下，红黑树作为一种查询性能较高的数据结构作为兜底，维持到O(logn) 6、动态扩容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // oldCapacity:旧哈希表的大小 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 扩容阈值 int oldThr = threshold; // newCapacity:新哈希表的大小，newThreshold:新的扩容阈值，大佬是不喜欢长命名么。。 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 旧哈希表大小 &gt;= 2^30，直接到顶INT_MAX if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 新哈希表大小 * 2 &lt; 2^30 且 旧哈希表大小 &gt;= 16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 扩容阈值 * 2 newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 旧哈希表大小为0，旧扩容阈值&gt;0，初始化为旧扩容阈值 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 旧哈希表大小为0，旧扩容阈值为0，初始化为默认值 newCap = DEFAULT_INITIAL_CAPACITY; //16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); // 16*0.75=12 &#125; // 新扩容阈值为0 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; // 容量*负载因子 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 将旧的哈希表元素重新散列到新的哈希表 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 该哈希槽只有一个元素 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 该哈希槽内对应的是红黑树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 该哈希槽内对应的是链表 else &#123; // preserve order // 位置不变的链表 Node&lt;K,V&gt; loHead = null, loTail = null; // 位置+oldCap的链表 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 为0则还在原位置 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 需要移动+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 位置不变的链表不为空，原位置指向头节点 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 位置+oldCap的链表不为空，原位置+oldCap指向头节点 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 扩容策略是当哈希表中的元素个数大于哈希表大小 * 负载因子，哈希表大小*2，乘2很好理解，因为保持了2的幂次方，可以看到扩容后需要对原来的元素进行重新散列，这个过程是非常消耗性能的。 负载因子默认为0.75，哈希表默认大小为16，当插入第13个元素时，哈希表扩容至32。 负载因子越大，空间利用率越高，哈希冲突可能性也越大，所以0.75是这两者考量之下折中的值，并且在这个情况下，哈希冲突同一位置超过8个的概率已经低于千万分之一了 hash函数：hash &amp; (newCap - 1)，即掩码多了一位1，那么要么还在原位置，要么移动原哈希表大小的位置（+oldCap） 7、对比JDK1.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 共享的空Mapstatic final Entry&lt;?,?&gt;[] EMPTY_TABLE = &#123;&#125;;// table就是HashMap实际存储数组的地方transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;public V put(K key, V value) &#123; // 当插入第一个元素的时候，需要先初始化数组大小 if (table == EMPTY_TABLE) &#123; // 数组初始化 inflateTable(threshold); &#125; // 如果key为null，将这个entry放到table[0]中 if (key == null) return putForNullKey(value); // 1. 求key的hash值，进行了多次异或，由于边际效应并不高，1.8只对高16位异或一次 int hash = hash(key); // 2. 找到对应的数组下标 int i = indexFor(hash, table.length); // 3. 遍历一下对应下标处的链表，看是否有重复的key已经存在，如果有，直接覆盖，put方法返回旧值就结束了 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; // key -&gt; value V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 4. 不存在重复的key，将此 entry 添加到链表中 addEntry(hash, key, value, i); return null;&#125;void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果当前HashMap大小已经达到了阈值，并且新值要插入的数组位置已经有元素了，那么要扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 扩容，容量 * 2 resize(2 * table.length); // 扩容以后，重新计算 hash 值 hash = (null != key) ? hash(key) : 0; // 重新计算扩容后的新的下标 bucketIndex = indexFor(hash, table.length); &#125; // 创建元素 createEntry(hash, key, value, bucketIndex);&#125;// 头插法，将新值放到链表的表头，然后size++void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K, V&gt; e = table[bucketIndex]; // 新节点.next = e table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125;void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; // 如果之前的HashMap已经扩充到最大了，那么就将临界值threshold设置为最大的int值 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新的数组 Entry[] newTable = new Entry[newCapacity]; // 将原来数组中的值迁移到新的更大的数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; // 阈值计算 threshold = (int) Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 8、总结Q1：江湖规矩，请问HashMap是线程安全的吗？为什么线程不安全？ Q2：JDK1.7和1.8的HashMap有哪些区别？ ​ Q1、Q2一起回答，当然是线程不安全的； ​ JDK1.7的HashMap添加元素发生哈希冲突时用的是头插法，即插入的元素作为头，这会使得扩容的时候链表逆序，此时多线程可能会产生循环链表，查询一个hash值对应这个循环链表而key又不存在的元素的时候就会陷入死循环中，JDK1.8使用尾插法（preserve order）避免了这种情况。 ​ 当然，JDK1.8的HashMap也是线程不安全的，最简单的一个栗子，if (++size &gt; threshold)，size是一个int类型的成员变量，没有原子性，很显然是线程不安全的，更糟糕的情况还有，如果两个哈希值相同，key不同的元素同时添加，可能会出现一个将另一个直接替换，就导致了数据的丢失。 ​ JDK1.7和1.8的HashMap的区别从数据存储结构来看，1.8多了红黑树的结构，会在哈希表总数&gt;=64 且 同一哈希槽节点数&gt;=8的时候从链表转换为红黑树，红黑树节点&lt;=6时转换成链表，中间隔了个7可以起到一定的缓冲作用。 ​ 1.8还做了一些优化，如hash()只进行了一次异或操作 Q3：如何解决Hash冲突解决？ ​ 上面已经说了，一种是链表，一种是红黑树 Q4：动态扩容的策略是什么？ ​ 指定大小初始化时，哈希表的大小为&gt;=指定大小的2的幂次方 ​ 当size超过扩容阈值（哈希表大小 * 负载因子）时，扩容至原来的两倍，将旧的哈希表数据重新散列到新的哈希表 ​ JDK1.8还有数据量小于64但某个哈希槽数据量&gt;=8时，也会进行扩容","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之LinkedList","slug":"Java源码之LinkedList","date":"2020-02-26T02:49:24.803Z","updated":"2020-02-26T07:40:41.854Z","comments":true,"path":"2020/02/26/Java源码之LinkedList/","link":"","permalink":"https://github.com/linlinnn/2020/02/26/Java%E6%BA%90%E7%A0%81%E4%B9%8BLinkedList/","excerpt":"","text":"Java源码之LinkedList1、带着问题看源码Q1：添加、删除、查找、遍历操作相应操作的原理是什么，时间复杂度是多少？ Q2：跟ArrayList的对比？ Q3：序列化机制？ 2、数据存储结构LinkedList的数据存储结构是一个双向链表 123456789101112131415transient int size = 0;transient Node&lt;E&gt; first; // 指向第一个节点transient Node&lt;E&gt; last; // 指向最后一个节点// 节点private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; // 指向下一个节点 Node&lt;E&gt; prev; // 指向上一个节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; size表示当前链表的节点数，显然可以根据size将链表分成两半，靠近首部就从first开始遍历，靠近尾部就从last开始遍历 3、初始化有两种初始化方法，无参初始化，指定数据集合初始化 1234567public LinkedList() &#123;&#125;public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c);&#125; 4、添加元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public boolean add(E e) &#123; // 在尾部添加一个元素 linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; // 最后一个节点为空，即first节点=last节点 if (l == null) first = newNode; // 添加到最后一个节点的后面 else l.next = newNode; size++; modCount++;&#125;// index表示第一个元素插入的位置public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; // 边界检查是否在[0,size] checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; // 添加元素集合长度为0，直接返回false if (numNew == 0) return false; Node&lt;E&gt; pred, succ; // 前驱节点、后继节点 // 在链尾添加，前驱节点为last节点 if (index == size) &#123; succ = null; pred = last; &#125; else &#123; // 否则，后继节点为index位置所在的节点 succ = node(index); pred = succ.prev; &#125; // 遍历集合插入元素到前驱节点的后面 for (Object o : a) &#123; @SuppressWarnings(\"unchecked\") E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); // 前驱节点为null,插入到链表首部 if (pred == null) first = newNode; // 否则，插入到前驱节点的后面 else pred.next = newNode; // 更新前驱节点 pred = newNode; &#125; // 连接后继节点 // 后继节点为空，即前驱节点为最后一个节点 if (succ == null) &#123; last = pred; &#125; else &#123; // 否则，连接在一起 pred.next = succ; succ.prev = pred; &#125; // 同步链表长度 size += numNew; modCount++; // 修改计数只+1 return true;&#125; 链表操作，边界情况需要仔细考虑，主要考虑头和尾，比如addAll 的逻辑如下 找到index位置对应的前驱节点和后继节点 1.1. 插入到链表尾部，后继节点为null 1.2. 插入到链表头部，前驱节点为null 1.3. 插入到链表中间，后继节点为index节点 将元素集合依次连接在前驱节点的后面 需要考虑1.2这种情况，前驱节点为null，即最后一个新增元素为first节点，然后依次连接 最后一个新增元素与后继节点相连 需要考虑1.1这种情况，后继节点为null，即最后一个新增元素为last节点 5、删除元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 删除头节点private E unlinkFirst(Node&lt;E&gt; f) &#123; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC // 更新头节点为下一个节点 first = next; // 链表只剩一个节点，删除后，first = last = null if (next == null) last = null; // 头节点的prev设为null else next.prev = null; size--; modCount++; return element;&#125;// 删除最后一个节点private E unlinkLast(Node&lt;E&gt; l) &#123; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC // 更新头节点为上一个节点 last = prev; // 链表只剩一个节点，删除后，first = last = null if (prev == null) first = null; // 尾节点的next设为null else prev.next = null; size--; modCount++; return element;&#125;// 删除一个非空节点E unlink(Node&lt;E&gt; x) &#123; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // 和前驱节点断开，考虑删除是不是头节点 if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; // 和后继节点断开，考虑删除是不是尾节点 if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; // 此时该节点的prev、item、next都设为了null size--; modCount++; return element;&#125; 6、查询元素1234567891011121314Node&lt;E&gt; node(int index) &#123; // index位于链表的左半部分，从first开始遍历 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 7、序列化机制12345678910111213141516171819private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException &#123; s.defaultWriteObject(); s.writeInt(size); // 按顺序序列化数据 for (Node&lt;E&gt; x = first; x != null; x = x.next) s.writeObject(x.item);&#125;@SuppressWarnings(\"unchecked\")private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; s.defaultReadObject(); int size = s.readInt(); // 将数据进行反序列化构建链表 for (int i = 0; i &lt; size; i++) linkLast((E)s.readObject());&#125; 8、总结Q1：添加、删除、查找、遍历操作相应操作的原理是什么，时间复杂度是多少？ ​ 添加/删除元素，时间复杂度与位置相关，越靠近头节点或者尾节点，则趋向或等于O(1)，最差就是在中间位置，需要遍历n/2的长度，查找同理；所以遍历需要注意的是要使用迭代器依次顺序遍历，否则如果使用下标随机访问，则每次都要从头节点或者尾结点遍历一遍，大大降低性能。 Q2：跟ArrayList的对比？ 一个是数组结构，一个是双向链表； ArrayList添加数据空间不够时需要动态扩容，LinkedList只需要将数据串联起来 都可以进行随机访问，但是ArrayList才适合，性能比LinkedList高得多 不能笼统地认为插入删除就链表快，查询遍历就数组快，还要考虑具体的位置 把数据插入到头部，LinkedList很快就找到位置并串联起来，但是ArrayList要将元素后移，所以LinkedList快，数据位置越靠近中间LinkedList效率就越差。 把数据插入到尾部，ArrayList需要分情况讨论，不需要扩容的情况下是效率是很高的，因为不用复制移动元素，相比而言LinkedList定位尾部很快，不过需要new对象和指针串连，效率慢了一点，删除同理 查询的话主要是LinkedList不适合随机访问，顺序访问的前提下两者相差不大，ArrayList稍稍占优 Q3：序列化机制？ ​ 双向链表方便于往前和往后遍历，不过也需要花费前驱节点、后继节点这些指针定位空间，而我们真正需要的只是具有顺序性的数据，所以序列化时免去了指针这些不必要的浪费，然后反序列化重新构建链表","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java源码之ArrayList","slug":"Java源码之ArrayList","date":"2020-02-25T01:14:10.654Z","updated":"2020-03-03T15:40:42.823Z","comments":true,"path":"2020/02/25/Java源码之ArrayList/","link":"","permalink":"https://github.com/linlinnn/2020/02/25/Java%E6%BA%90%E7%A0%81%E4%B9%8BArrayList/","excerpt":"","text":"Java源码之ArrayList1、带着问题看源码Q1：添加、查找、遍历操作最为普遍，相应操作的原理是什么，时间复杂度是多少？ Q2：如何进行动态扩展的？ Q3：序列化机制是怎样的？ 2、数据存储结构ArrayList在日常工作中非常常用，底层结构就是一个数组 12345678private static final int DEFAULT_CAPACITY = 10;private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;transient Object[] elementData; // non-private to simplify nested class accessprivate int size; 1、DEFAULT_CAPACITY，初始大小为10 2、EMPTY_ELEMENTDATA 和DEFAULTCAPACITY_EMPTY_ELEMENTDATA 的区别在于添加第一个元素时的扩容策略 3、elementData使用transient修饰，比如一个100万大小的数组，只存储了100个数据，那么只序列化这100个就好了，避免浪费不必要的资源，使用writeObject和readObject进行序列化 3、初始化有三种初始化方法：指定大小初始化，无参初始化，指定数据集合初始化 12345678910111213141516171819202122232425262728293031public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; // 创建一个指定大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; // 返回静态的空数组EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; // 初始大小为负数，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125;public ArrayList() &#123; // 返回静态的空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA，在添加第一个元素后大小才为10 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) // 如果集合元素类型不是Object，转换成Object类型，Q4：为什么？ if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 返回静态的空数组EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; Q4的测试代码如下，转换是为了防止数组元素赋值时发生类型错误 123456List&lt;String&gt; list = Arrays.asList(\"hello,ArrayList\");Object[] arr = list.toArray();System.out.println(arr.getClass().getSimpleName()); // String[]arr[0] = new Object();// java.lang.ArrayStoreException 4、添加元素1234567891011121314151617181920212223242526272829303132333435363738public boolean add(E e) &#123; // 确保数组容量大小，不够时执行扩容 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 如果是静态空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA，在minCapacity和默认大小10取最大值 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 当前数组大小小于期望大小，数组需要扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; // 扩容加上原来的一半大小，即原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 加上一半还是不够大的话就直接扩至期望大小 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 准备扩容大小超过的INT_MAX - 8 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // 为负时说明超出了INT的范围，抛出OutOfMemoryError异常 // 否则MAX_ARRAY_SIZE刚好则为MAX_ARRAY_SIZE，还不够就INT_MAX newCapacity = hugeCapacity(minCapacity); // 将原数组拷贝到新的数组 elementData = Arrays.copyOf(elementData, newCapacity);&#125; 5、删除元素12345678910111213141516171819202122232425public E remove(int index) &#123; // 数组边界检查，只对上限做了检查 rangeCheck(index); // 修改计数+1 modCount++; E oldValue = elementData(index); // 把删除的元素以后的元素向前移动 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125;// Checks if the given index is in range. If not, throws an appropriate runtime exception.private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; ​ 这里值得注意的是rangeCheck和rangeCheckForAdd 的区别，函数的意义决定了函数的职责边界，从而对应内部的实现，如rangeCheck 是get、remove、set方法操作已存在元素的，所以只检查上边界，下边界检查的职责交给数组的访问，而rangeCheckForAdd 是add、addAll操作未存在元素的，所以检查上下边界。 6、迭代器的remove和ArrayList的remove123456789101112131415161718192021222324252627282930public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; // 同步期望计数 expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125;&#125;// 修改计数和期望计数不相同，抛出异常final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125;private void fastRemove(int index) &#123; // 修改计数+1，后续没有进行同步期望计数，在遍历过程中会抛出ConcurrentModificationException modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 遍历删除元素时两种方法比较 通过迭代器Iterator#remove方法不会报错，而forEach调用至fastRemove由于没有同步期望计数，会抛出ConcurrentModificationException 所以这两个remove方法有一定的偏向性，即ArrayList的remove应该用于删除单个元素的场景 7、序列化机制12345678910111213141516171819202122232425262728293031323334353637383940414243private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; int expectedModCount = modCount; // 序列化non-static和non-transient的数据 s.defaultWriteObject(); // Q5:为什么这里还要write一次size呢？ // 这是为了版本兼容，老版本根据size这个成员变量去申请对应空间 // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // 序列化数组元素 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 8、总结Q1：添加、查找、遍历操作最为普遍，相应操作的原理是什么，时间复杂度是多少？ ​ 添加/删除操作，需要考虑具体的位置，在数组开头，需要移动元素，时间复杂度是O(n)，在数组尾部，时间复杂度是O(1)，遍历查找无序数组中的一个元素时间复杂度为O(n) Q2：如何进行动态扩展的？ ​ 当数组空间不足时进行动态扩展，扩展到原数组的1.5倍大小，仍不够的话扩展到期望大小（这种情况是初始化大小为10时或者addAll时发生），最多扩展至INT_MAX Q3：序列化机制是怎样的？ ​ 保存元素的element数组使用transient修饰，是避免序列化没有存储数据的空间提升性能，使用定制化的writeObject序列化和readObject反序列化","categories":[],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"https://github.com/linlinnn/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Dubbo扩展点加载机制","slug":"dubbo扩展点","date":"2020-01-11T16:23:25.855Z","updated":"2020-01-19T16:29:03.086Z","comments":true,"path":"2020/01/12/dubbo扩展点/","link":"","permalink":"https://github.com/linlinnn/2020/01/12/dubbo%E6%89%A9%E5%B1%95%E7%82%B9/","excerpt":"","text":"Dubbo扩展点加载机制1、Java SPI使用了策略模式，一个接口多种实现。只声明接口，具体的实现由程序之外的配置控制，用于具体实现的装配。 具体步骤如下： （1）定义一个接口以及对应的方法 （2）编写接口的实现类 （3）在META-INF/services/ 目录下，创建一个接口全限定名命名的文件 （4）文件内容为具体实现类的全限定名，如果有多个，则用分行符分隔 （5）在代码中通过java.util.ServiceLoader 来加载具体的实现类 2、扩展点加载机制的改进 初始化 JDK SPI: 一次性实例化扩展点所有实现，初始化耗时，没有也加载浪费资源 Dubbo SPI: 加载配置文件中的类，并分为不同的种类缓存在内存中，不会立即全部初始化 扩展点加载失败 JDK SPI: 获取不到扩展的名称，不能打印正常的异常信息 Dubbo SPI: 抛出真实异常并打印日志，部分扩展点加载失败不会影响其他扩展点和整个框架的使用 实现了IOC和AOP机制 3、扩展点的配置规范 规范名 规范说明 SPI配置文件路径 META-INF/services/、META-INF/dubbo/、META-INF/dubbo/internal/ 全路径类名 文件内容格式 key=value方式，多个用换行符分隔 4、扩展点的分类与缓存Dubbo SPI Class缓存：Dubbo SPI获取扩展类时，先从缓存中读取。如果缓存中不存在，则加载配置文件，根据配置把Class缓存到内存中，不会直接初始化 实例缓存：基于性能考虑，Dubbo框架不仅会缓存Class，也会缓存Class实例化对象。先从缓存中读取，如果缓存中不存在，则重新加载并缓存起来，按需实例化并缓存 扩展类种类 普通扩展类 包装扩展类：Wrapper类没有具体的实现，只是做了通用逻辑的抽象，在构造方法中传入一个具体的扩展接口的实现 自适应扩展类：一个扩展接口有多种实现类，具体实现哪个实现类可以不写死在配置或代码中，在运行时，通过传入URL中的某些参数动态来确定。自适应特性@Adaptive 其他缓存 自适应和自动激活的区别？ isAssignableFrom 和 instanceof 的区别？ 123父类.class.isAssignableFrom(子类.class)子类实例 instanceof 父类类型","categories":[],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://github.com/linlinnn/tags/Dubbo/"}]}]}